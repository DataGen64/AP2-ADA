{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### FIT5149 - Applied Data Analysis - Assignment 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### INSTALL PACKAGES"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting emoji\n",
      "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/40/8d/521be7f0091fe0f2ae690cc044faf43e3445e0ff33c574eae752dd7e39fa/emoji-0.5.4.tar.gz (43kB)\n",
      "\u001b[K    100% |████████████████████████████████| 51kB 1.5MB/s ta 0:00:01\n",
      "\u001b[?25hBuilding wheels for collected packages: emoji\n",
      "  Building wheel for emoji (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25h  Stored in directory: /Users/akspikey/Library/Caches/pip/wheels/2a/a9/0a/4f8e8cce8074232aba240caca3fade315bb49fac68808d1a9c\n",
      "Successfully built emoji\n",
      "Installing collected packages: emoji\n",
      "Successfully installed emoji-0.5.4\n"
     ]
    }
   ],
   "source": [
    "# !pip install emoji\n",
    "#!pip install gensim"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### IMPORT PACKAGES"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from nltk.corpus import stopwords\n",
    "from nltk import word_tokenize    \n",
    "from nltk.tokenize import wordpunct_tokenize\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.metrics import precision_score, recall_score, f1_score, accuracy_score, confusion_matrix, matthews_corrcoef\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.naive_bayes import MultinomialNB, GaussianNB,BernoulliNB\n",
    "from sklearn.svm import LinearSVC, SVC\n",
    "from sklearn.ensemble import RandomForestClassifier, StackingClassifier\n",
    "import xml.etree.ElementTree as ET\n",
    "from sklearn import preprocessing\n",
    "import re, string\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.model_selection import train_test_split\n",
    "import emoji\n",
    "from sklearn.preprocessing import scale, StandardScaler\n",
    "import gensim.downloader as api\n",
    "from nltk.tokenize import TweetTokenizer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### PREPARING THE DATA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_labels = pd.read_csv(\"train_labels.csv\")\n",
    "test_data = pd.read_csv(\"test.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>gender</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>d7d392835f50664fc079f0f388e147a0</td>\n",
       "      <td>male</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>ee40b86368137b86f51806c9f105b34b</td>\n",
       "      <td>female</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>919bc742d9a22d65eab1f52b11656cab</td>\n",
       "      <td>male</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>15b97a08d65f22d97ca685686510b6ae</td>\n",
       "      <td>female</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>affa98421ef5c46ca7c8f246e0a134c1</td>\n",
       "      <td>female</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3095</td>\n",
       "      <td>97159e619b8d88bdd837f7f7e738de43</td>\n",
       "      <td>male</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3096</td>\n",
       "      <td>9bccadb3d0033a2b2ad4403184ea72f5</td>\n",
       "      <td>female</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3097</td>\n",
       "      <td>f252cb406d4c27e71414148175fe6878</td>\n",
       "      <td>female</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3098</td>\n",
       "      <td>5dcf483c6ceb4cdf9de1648486f28706</td>\n",
       "      <td>female</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3099</td>\n",
       "      <td>5e877ae08d481609c0a828aaa2ba9bb0</td>\n",
       "      <td>male</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>3100 rows × 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                    id  gender  label\n",
       "0     d7d392835f50664fc079f0f388e147a0    male      1\n",
       "1     ee40b86368137b86f51806c9f105b34b  female      0\n",
       "2     919bc742d9a22d65eab1f52b11656cab    male      1\n",
       "3     15b97a08d65f22d97ca685686510b6ae  female      0\n",
       "4     affa98421ef5c46ca7c8f246e0a134c1  female      0\n",
       "...                                ...     ...    ...\n",
       "3095  97159e619b8d88bdd837f7f7e738de43    male      1\n",
       "3096  9bccadb3d0033a2b2ad4403184ea72f5  female      0\n",
       "3097  f252cb406d4c27e71414148175fe6878  female      0\n",
       "3098  5dcf483c6ceb4cdf9de1648486f28706  female      0\n",
       "3099  5e877ae08d481609c0a828aaa2ba9bb0    male      1\n",
       "\n",
       "[3100 rows x 3 columns]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Encode Labels\n",
    "le = preprocessing.LabelEncoder()\n",
    "train_labels['label'] = le.fit_transform(train_labels.gender.values)\n",
    "train_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def parse_training_docs(data):\n",
    "    train_data = data\n",
    "    for index, row in train_labels.iterrows():\n",
    "        doc_string = \"\"\n",
    "        tree = ET.parse('data/'+row['id']+'.xml')\n",
    "        document = tree.getroot().find(\"documents\").findall(\"document\")\n",
    "        for doc in document:\n",
    "            doc_string = doc_string + \" \" + doc.text\n",
    "        train_labels.loc[index,'document'] = doc_string\n",
    "    return train_data\n",
    "\n",
    "train_labels = parse_training_docs(train_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>gender</th>\n",
       "      <th>label</th>\n",
       "      <th>document</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>d7d392835f50664fc079f0f388e147a0</td>\n",
       "      <td>male</td>\n",
       "      <td>1</td>\n",
       "      <td>@CSIFERROSCAN youch! Good things to know! Is ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>ee40b86368137b86f51806c9f105b34b</td>\n",
       "      <td>female</td>\n",
       "      <td>0</td>\n",
       "      <td>Donald the Menace #ThanksComey  https://t.co/...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>919bc742d9a22d65eab1f52b11656cab</td>\n",
       "      <td>male</td>\n",
       "      <td>1</td>\n",
       "      <td>This seems super sketch / too good to be true...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>15b97a08d65f22d97ca685686510b6ae</td>\n",
       "      <td>female</td>\n",
       "      <td>0</td>\n",
       "      <td>Just some texts with my dad about our Saturda...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>affa98421ef5c46ca7c8f246e0a134c1</td>\n",
       "      <td>female</td>\n",
       "      <td>0</td>\n",
       "      <td>Irrevocably love this talented human and so p...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                 id  gender  label  \\\n",
       "0  d7d392835f50664fc079f0f388e147a0    male      1   \n",
       "1  ee40b86368137b86f51806c9f105b34b  female      0   \n",
       "2  919bc742d9a22d65eab1f52b11656cab    male      1   \n",
       "3  15b97a08d65f22d97ca685686510b6ae  female      0   \n",
       "4  affa98421ef5c46ca7c8f246e0a134c1  female      0   \n",
       "\n",
       "                                            document  \n",
       "0   @CSIFERROSCAN youch! Good things to know! Is ...  \n",
       "1   Donald the Menace #ThanksComey  https://t.co/...  \n",
       "2   This seems super sketch / too good to be true...  \n",
       "3   Just some texts with my dad about our Saturda...  \n",
       "4   Irrevocably love this talented human and so p...  "
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_labels.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'s at the Aus Open. Latecomers better get in quick, Fed’s up 5-0 already in 13 mins 😳 What makes this horse shit weather worse is watching the Aus Open with Melbourne bathed in sunshine 😫 #Summer2017 @ThornhillDave So do you want this now for your birthday instead of the Miranda Sex Arse? @ThornhillDave What weatherbomb??! https://t.co/VC93gMCzK5 So *someone* thought it was a good idea to tackle the weather bomb this morning… https://t.co/nc34ECgsiA Back to the grind tomorrow. Having successfully only worn them 3 times since Christmas, wearing socks is going to be the biggest shock! @SKYNZ WiFi on iPad Pro &amp; iPad Air. iPhone actually works fine. @SKYNZ Any issues with SKYGO app? Getting this on multiple devices https://t.co/a2xKZZESXp @justin_lester Just tell them you know someone who knows the mayor… @justin_lester Make sure you add the hashtag #NBAVote 😉 @RealStevenAdams Damn. Last 2 points for @RealStevenAdams cost me at the TAB! #NBAVote https://t.co/2RnwfinC8z @Timely Nice! So a walk-in service doesn’t affect rebooking stats? @ThornhillDave You don’t need to see a picture of my stiffy @ThornhillDave Drew a picture… 😂 Cards of Humanity with Mum: “What’s a stiffy?” @BR4DY Haha I was wondering perhaps that hotel in Greytowb was putting on a performance…?! 😂 Middle of summer &amp; still snow on Mt Ruapehu #climatechange https://t.co/YJgDxEoG7Q @brendontrass Which channel? @ThornhillDave 😮 He’d be exhausted after that… My 2017 will be more interesting to the game but it’s a bit too good 😊 https://t.co/C1TGINztNJ @jptocker Is it as good as your accountant? 😂😂 @BrentfromNZ @jessbovey Think they’re closed until Feb due to Reading car park demo @jessbovey Martinborough? Bloody typical. My hay fever always kicks in right at the end of The Notebook… Statue of Liberty Jam! @RealStevenAdams #NBAVote https://t.co/sSZYOHtUxq So took a leaf from the big fella today, checked it twice, nailed it 👌🏽 Adulting ftw https://t.co/N2Nkn6YSre @jessbovey The struggle is real! Just discovered that I put my undies on inside out today. Is it Christmas yet 😫 Keeping fingers crossed that next #KFCAdventCalendar present is that they\\'re moving back to Manners Mall #byecottonon .@kfcnz Oh all the shades of dark grey once I\\'ve wiped my hands on them after demolishing a bucket #KFCAdventCalendar @jessbovey Take them all to town &amp; give to the Mary Potter Hospice people to wrap for gold coin donation 💁🏻\\u200d♂️ Biting the bullet and buying NBA league pass if only so I never have to hear the ESPN 12 days of Christmas ad again 😳 Great addon to an awesome event. CBD is buzzing. Starting to feel like Christmas 🎄 https://t.co/66Ij9Fa6QR @KiwibankNZ Thanks guys. I\\'ll check the postbox this afternoon and if no dice, then I\\'ll get in contact. I know why you had to @KiwibankNZ but cancelling cc &amp; still waiting a week later for new one is most inconvenient as all cloud subs bouncing @AbbieThornhill @jessbovey 7 inches normally is… @LloydPercival @jptocker @kfcnz They sure do! Or did… https://t.co/Rqlkf6t27h New leader of National Party will be decided via contest on the Edge singing Mariah\\'s All I Want For Christmas #KeyResigns Bad news comes in threes... https://t.co/8vpavC3CTw What more can this day throw at us! https://t.co/9tkSNqocYR 6th ball of the match, Tubby pulls out the “shot of the match” call. #AUSvNZL @TheACCnz Looks like Twitter has decided we\\'re swapping two Harvies for an Earl... @TweetingTrace @JayHarvie @AliceCooke3 https://t.co/u9uqYhmCYA @WhittakersNZ 11 #tweats @jessbovey \"NOTICE: This domain name expired on 11/25/2016 and is pending renewal or deletion.   Renew It Now!\" @nickyoung210 Did you not have a yahoo moment.... The TV2 Sunday “Premiere” movie is Mr Deeds. A movie released in 2002…. @TashTasticNZ Yeah, it was a torn medial meniscus. Just a minor op though - so they say! @TashTasticNZ It’s both of us!! I’m getting my knee done today as well! Nil by mouth until 4pm starts now. This is gonna be ugly… I must be the only person to ever injure their knee again the night before a knee operation 😩 Here’s hoping both injuries are related! Great innovative idea! https://t.co/gBmePwn7YO Mark Reason take note, this is why Beauden Barrett is starting. Also why you’re not the All Black coach #IREvNZL @LloydPercival Haha! All I see are beards! 🎅🏽 Great airline banter. Check out the whole thread. Plus some mad photoshop skills! https://t.co/hFX2Hb4JSj Or is it because there are so many beards in Welly it’s basically Movember all year round? https://t.co/27iZ0Z9m58 Taking a break from earthquakes for a mo, so is Movember still a thing? Saw one dirty tache today &amp; it is probably the only one I’ve seen! It’s so frickin embarrassing that Brian Tamaki is a NZer. How does the Queen manage to get a letter sent here in 2 days yet my ASOS order is still \"in transit\" #eqnz Ok... hands up who else said about 10 mins ago that we hadn\\'t had a strong aftershock for a while #eqnz Nope nope nope nope! Cmon! We’ve JUST cleaned up my mothers apartment #eqnz @LloydPercival You working with Jipa?! Great, just as we thought the aftershocks had died down and dared to go to bed #eqnz @007GAL oh what?!! #jealous @CookieTimeNZ Breaking out the earthquake snack supplies. I knew we should have bought that bucket of Cookie Time Christmas Cookies today! #eqnz @LloydPercival Every aftershock has us clenching and gritting thinking it will go for longer :/ Hope kids both ok #eqnz @JayHarvie Lucky you didn\\'t bite into that Camel Grill today then wasn\\'t it! @AbbieThornhill @JayHarvie This has been @AbbieThornhill\\'s worse nightmare for the past half hour. She has a turtle head.... @KeithQuinn88 We lost some chinaware up on 14! https://t.co/vQHRn6hW1E Link to keep track of updates https://t.co/7EK3PqpA8m #eqnz Aftershocks are not fun at all #eqnz Thought that was it. 14 floors up and it didn’t seem like it was going to stop shaking. #eqnz @GregSheehanRW I had the same feelings yesterday morning when it was 33-29 (\"it\\'ll be fine\"). Not sure about anything anymore! @ClareCapital Helps if ALB wasn’t presenting the after match for Sky! 😃 https://t.co/7ijhP1WZQA https://t.co/5yT11grsJX Can’t take anything away from Ireland’s performance but shows how important Retallick is to this ABs side #NZLvIRE Pray for @brendontrass https://t.co/gc9x9vjYCY Meanwhile on the other channel 😫😫 #NZLvIRE https://t.co/ZooG12UqPu Ardie onto the wing? #NZLvIRE The Barrett father will be smiling…. #NZLvIRE @JayHarvie ah well. Some days you are like that. You get knocked down. But all you can do is get up again @jessbovey I’m picking girl or boy 👨\\u200d👩\\u200d👧\\u200d👦👨\\u200d👩\\u200d👦\\u200d👦 @JulieReddish Albeit slightly illegal! I mean seriously. What kind of world are we living in here? The big question which stems from this article is why the heck does Hastings have a Carl\\'s Jnr and we don\\'t?!! https://t.co/Dt8bGchH8G That moment Jesus Shuttlesworth killed my beloved @spurs #rayray https://t.co/OI6INcKEc1 @sulufiti They want the free shit! Not gonna get $140m in one hit and will also be taxed but cool story bro. 👍🏽 https://t.co/ClXo3sWZbW @AbbieThornhill I\\'ll be home in 2 secs... @AbbieThornhill That is true. As a naturally warm person, this house on Grand Designs sounds like my worst nightmare!! I’d be living in my undies!'"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_labels.iloc[0][\"document\"][1000:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re, string\n",
    "def clean_documents(document):\n",
    "    # Remove square brackets characters\n",
    "#     document = re.sub(r'\\[.*?\\]', '', document)\n",
    "    #Remove URL\n",
    "    document = re.sub(r'http\\S+', '', document)\n",
    "    #Remove user mentions\n",
    "    document = re.sub(r\"@(\\w+)\", ' ', document, flags=re.MULTILINE)\n",
    "    #Remove punctuations\n",
    "    document = re.sub(r'[%s]' % re.escape(string.punctuation), '', document)\n",
    "    #Remove words with numbers\n",
    "    document = re.sub(r'\\w*\\d\\w*', '', document)  \n",
    "    #Remove triple dots\n",
    "    document = document.replace(\"…\", \" \")\n",
    "    #Remove Quotes\n",
    "    document = document.replace(\"'\", \"\")\n",
    "    document = document.replace(\"\\\"\", \"\")\n",
    "    document = document.replace(\"’\", \"\")\n",
    "    document = document.replace(\"“\", \"\")\n",
    "    document = document.replace(\"”\", \"\")\n",
    "    #convert to lowercase\n",
    "    document = document.lower()\n",
    "    return document\n",
    "train_labels['document'] = train_labels['document'].apply(lambda doc: clean_documents(doc))\n",
    "# train_labels[\"document\"].values.tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data, validation_data = train_test_split(train_labels, test_size=0.2, random_state=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The number of Training documents are:  2480\n",
      "The number of Validation documents are:  620\n"
     ]
    }
   ],
   "source": [
    "print(\"The number of Training documents are: \", len(train_data))\n",
    "print(\"The number of Validation documents are: \", len(validation_data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "from matplotlib import pyplot as plt\n",
    "train_data.label.value_counts().sort_index(ascending=False).plot(kind='bar')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize_and_lemmatize(doc):\n",
    "    stopwords_list = stopwords.words('english')\n",
    "    wnl = WordNetLemmatizer()\n",
    "    tknzr = TweetTokenizer()\n",
    "    doc = tknzr.tokenize(doc)\n",
    "    doc = [ wnl.lemmatize(token) for token in doc if token not in stopwords_list and len(token) > 0]\n",
    "    doc = [ token for token in doc if len(token) > 1 or token in emoji.UNICODE_EMOJI.keys()]\n",
    "    return doc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_docs = [tokenize_and_lemmatize(doc) for doc in train_data[\"document\"] .tolist()]\n",
    "train_labels = train_data[\"label\"].tolist()\n",
    "\n",
    "validation_docs = [tokenize_and_lemmatize(doc) for doc in validation_data[\"document\"] .tolist()]\n",
    "validation_labels = validation_data[\"label\"].tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['sound',\n",
       " 'cool',\n",
       " 'eh',\n",
       " 'younguns',\n",
       " 'fashionable',\n",
       " 'language',\n",
       " 'lady',\n",
       " 'sewing',\n",
       " 'circle',\n",
       " 'shall',\n",
       " 'hear',\n",
       " 'amp',\n",
       " 'loved',\n",
       " '😀',\n",
       " 'wardinibooks',\n",
       " 'wonder',\n",
       " 'could',\n",
       " 'maybe',\n",
       " 'shit',\n",
       " 'got',\n",
       " 'real',\n",
       " 'finished',\n",
       " 'yr',\n",
       " 'book',\n",
       " 'sitting',\n",
       " 'wonderful',\n",
       " 'amp',\n",
       " 'powerful',\n",
       " 'stuff',\n",
       " 'please',\n",
       " 'write',\n",
       " 'chris',\n",
       " 'round',\n",
       " 'good',\n",
       " 'guy',\n",
       " 'tragedy',\n",
       " 'lol',\n",
       " 'realized',\n",
       " 'new',\n",
       " 'handlefacepalm',\n",
       " 'napier',\n",
       " 'north',\n",
       " 'island',\n",
       " 'brings',\n",
       " 'welcome',\n",
       " 'nz',\n",
       " 'fun',\n",
       " 'drink',\n",
       " 'wine',\n",
       " 'amp',\n",
       " 'dont',\n",
       " 'mind',\n",
       " 'earthquake',\n",
       " '😀',\n",
       " 'cheer',\n",
       " 'lynz',\n",
       " '😀',\n",
       " 'always',\n",
       " 'fun',\n",
       " 'taken',\n",
       " 'phd',\n",
       " 'level',\n",
       " 'beautiful',\n",
       " '😌',\n",
       " 'agree',\n",
       " 'thank',\n",
       " 'gah',\n",
       " 'end',\n",
       " 'well',\n",
       " 'put',\n",
       " 'salute',\n",
       " 'roger',\n",
       " 'sir',\n",
       " 'stand',\n",
       " 'easy',\n",
       " 'till',\n",
       " 'well',\n",
       " 'done',\n",
       " 'kokako',\n",
       " 'sad',\n",
       " 'lovely',\n",
       " 'shore',\n",
       " 'plover',\n",
       " '😢',\n",
       " 'learningcurve',\n",
       " 'bro',\n",
       " 'legend',\n",
       " 'also',\n",
       " 'beer',\n",
       " 'good',\n",
       " 'replacement',\n",
       " 'fluid',\n",
       " 'sport',\n",
       " 'wonder',\n",
       " 'voted',\n",
       " 'boty',\n",
       " 'yet',\n",
       " 'shore',\n",
       " 'plover',\n",
       " 'win',\n",
       " '👍',\n",
       " '🏽',\n",
       " 'think',\n",
       " 'sprained',\n",
       " 'face',\n",
       " 'laughing',\n",
       " 'last',\n",
       " 'nightthanks',\n",
       " 'bill',\n",
       " '😂',\n",
       " 'aha',\n",
       " 'thank',\n",
       " 'spindly',\n",
       " 'lovely',\n",
       " 'one',\n",
       " 'right',\n",
       " 'maybe',\n",
       " 'mo',\n",
       " 'coombs',\n",
       " 'victoria',\n",
       " 'wgtn',\n",
       " 'moving',\n",
       " 'playground',\n",
       " 'rougher',\n",
       " 'others',\n",
       " 'im',\n",
       " 'homesick',\n",
       " 'whilst',\n",
       " 'home',\n",
       " 'confused',\n",
       " 'little',\n",
       " 'guy',\n",
       " 'made',\n",
       " 'remember',\n",
       " 'gran',\n",
       " 'parkin',\n",
       " 'euphemism',\n",
       " 'fantastic',\n",
       " 'also',\n",
       " 'issue',\n",
       " 'foamed',\n",
       " '😀',\n",
       " 'present',\n",
       " 'nz',\n",
       " 'cross',\n",
       " 'finger',\n",
       " 'look',\n",
       " 'amazing',\n",
       " 'plan',\n",
       " 'stream',\n",
       " 'nz',\n",
       " 'wiping',\n",
       " 'away',\n",
       " 'tear',\n",
       " 'thats',\n",
       " 'beautiful',\n",
       " 'work',\n",
       " 'gent',\n",
       " 'proud',\n",
       " 'gift',\n",
       " 'curse',\n",
       " 'celebrating',\n",
       " 'mclined',\n",
       " 'ortolana',\n",
       " 'lovely',\n",
       " 'day',\n",
       " 'slytherin',\n",
       " '😉',\n",
       " 'might',\n",
       " 'thunderstruck',\n",
       " 'probably',\n",
       " 'little',\n",
       " 'wee',\n",
       " 'get',\n",
       " 'see',\n",
       " 'welly',\n",
       " 'squee',\n",
       " 'ok',\n",
       " 'joke',\n",
       " 'would',\n",
       " 'reason',\n",
       " 'surely',\n",
       " 'thats',\n",
       " 'intention',\n",
       " 'notsureanymore',\n",
       " 'great',\n",
       " 'read',\n",
       " 'love',\n",
       " 'otrpod',\n",
       " 'thanks',\n",
       " 'funny',\n",
       " 'girl',\n",
       " 'makeup',\n",
       " 'mask',\n",
       " 'sketch',\n",
       " 'series',\n",
       " 'slap',\n",
       " 'frinjury',\n",
       " 'sound',\n",
       " 'like',\n",
       " 'adult',\n",
       " 'movie',\n",
       " 'actor',\n",
       " 'salad',\n",
       " 'amp',\n",
       " 'omelette',\n",
       " 'within',\n",
       " 'week',\n",
       " 'overachiever',\n",
       " 'ooh',\n",
       " 'yes',\n",
       " 'plasticine',\n",
       " 'lovely',\n",
       " 'beam',\n",
       " 'crayon',\n",
       " 'pls',\n",
       " 'chin',\n",
       " 'lovely',\n",
       " 'pup',\n",
       " 'make',\n",
       " 'thing',\n",
       " 'better',\n",
       " '🐶',\n",
       " 'yup',\n",
       " 'watch',\n",
       " 'space',\n",
       " 'yup',\n",
       " 'batten',\n",
       " 'hatch',\n",
       " 'one',\n",
       " 'know',\n",
       " 'omg',\n",
       " 'wireless',\n",
       " 'detail',\n",
       " 'please',\n",
       " 'agree',\n",
       " 'take',\n",
       " 'leaf',\n",
       " 'gandalfs',\n",
       " 'book',\n",
       " 'shall',\n",
       " 'pas',\n",
       " 'lovely',\n",
       " 'image',\n",
       " 'throw',\n",
       " 'head',\n",
       " 'back',\n",
       " 'laughing',\n",
       " 'help',\n",
       " 'please',\n",
       " 'surge',\n",
       " 'strap',\n",
       " 'tearing',\n",
       " 'away',\n",
       " 'around',\n",
       " 'tiny',\n",
       " 'screw',\n",
       " 'advise',\n",
       " 'oh',\n",
       " 'yes',\n",
       " 'required',\n",
       " 'reading',\n",
       " 'right',\n",
       " 'thanks',\n",
       " '👏',\n",
       " '🏽',\n",
       " 'well',\n",
       " 'done',\n",
       " 'nice',\n",
       " 'one',\n",
       " 'bartsalumni',\n",
       " 'lookout',\n",
       " 'book',\n",
       " 'howler',\n",
       " 'newspaper',\n",
       " 'deliverer',\n",
       " 'chambermaid',\n",
       " 'glass',\n",
       " 'washer',\n",
       " 'shoe',\n",
       " 'seller',\n",
       " 'tax',\n",
       " 'office',\n",
       " 'file',\n",
       " 'clerk',\n",
       " 'babysitter',\n",
       " 'doctor',\n",
       " 'essential',\n",
       " 'young',\n",
       " 'woman',\n",
       " 'young',\n",
       " 'men',\n",
       " 'thanks',\n",
       " '👍',\n",
       " 'dang',\n",
       " 'plenty',\n",
       " 'fluid',\n",
       " 'wine',\n",
       " 'count',\n",
       " 'take',\n",
       " 'care',\n",
       " 'oh',\n",
       " 'dear',\n",
       " 'yup',\n",
       " 'cool',\n",
       " 'cool',\n",
       " 'remembering',\n",
       " 'seeing',\n",
       " 'wembley',\n",
       " 'amp',\n",
       " 'deaf',\n",
       " 'day',\n",
       " '😜',\n",
       " 'found',\n",
       " 'youamp',\n",
       " 'love',\n",
       " 'writing',\n",
       " 'please',\n",
       " 'continue',\n",
       " '😀',\n",
       " 'transplant',\n",
       " 'recipient',\n",
       " 'strip',\n",
       " 'new',\n",
       " 'organ',\n",
       " 'donation',\n",
       " 'campaign',\n",
       " 'pure',\n",
       " 'brilliant',\n",
       " 'transplant',\n",
       " 'recipient',\n",
       " 'strip',\n",
       " 'new',\n",
       " 'organ',\n",
       " 'donation',\n",
       " 'campaign',\n",
       " 'thank',\n",
       " 'great',\n",
       " 'writing',\n",
       " 'found',\n",
       " 'twitter',\n",
       " 'somebody',\n",
       " 'say',\n",
       " 'bacon',\n",
       " 'sure',\n",
       " 'could',\n",
       " 'recruit',\n",
       " 'hb',\n",
       " 'listen',\n",
       " 'beautiful',\n",
       " '😀',\n",
       " 'yes',\n",
       " 'certain',\n",
       " 'point',\n",
       " 'randomly',\n",
       " 'decides',\n",
       " 'lovely',\n",
       " 'amy',\n",
       " 'short',\n",
       " 'amygdala',\n",
       " 'also',\n",
       " 'beloved',\n",
       " 'look',\n",
       " 'interrupted',\n",
       " 'reading',\n",
       " 'paper',\n",
       " 'venerable',\n",
       " 'amp',\n",
       " 'wise',\n",
       " '😀',\n",
       " 'need',\n",
       " 'photo',\n",
       " 'said',\n",
       " 'dog',\n",
       " 'sound',\n",
       " 'good',\n",
       " '😀',\n",
       " 'yoohoo',\n",
       " 'wasnt',\n",
       " 'time',\n",
       " 'keep',\n",
       " 'touch',\n",
       " 'hope',\n",
       " 'continue',\n",
       " 'talking',\n",
       " 'dunwell',\n",
       " 'etc',\n",
       " 'well',\n",
       " 'done',\n",
       " 'iceland',\n",
       " 'properfootball',\n",
       " 'bruised',\n",
       " 'arm',\n",
       " 'archery',\n",
       " 'wwgsd',\n",
       " 'weekend',\n",
       " 'thanks',\n",
       " 'amp',\n",
       " 'team',\n",
       " '😀',\n",
       " 'thanks',\n",
       " 'jess',\n",
       " 'spread',\n",
       " 'word',\n",
       " 'wwgsd',\n",
       " 'ok',\n",
       " 'there',\n",
       " 'brexit',\n",
       " 'gulp',\n",
       " 'bright',\n",
       " 'side',\n",
       " 'there',\n",
       " 'wwgsd',\n",
       " '😀',\n",
       " 'birthday',\n",
       " 'sweet',\n",
       " 'lovely',\n",
       " 'day',\n",
       " '😀',\n",
       " 'look',\n",
       " 'like',\n",
       " 'great',\n",
       " 'smacc',\n",
       " 'smaccwelly',\n",
       " 'boo',\n",
       " 'crikey',\n",
       " 'alex',\n",
       " 'peacenik',\n",
       " 'yeah',\n",
       " 'right',\n",
       " 'slap',\n",
       " 'sense',\n",
       " 'youre',\n",
       " 'welcome',\n",
       " 'breathe',\n",
       " 'paper',\n",
       " 'bag',\n",
       " 'bitch',\n",
       " 'ok',\n",
       " 'might',\n",
       " 'catch',\n",
       " 'thursday',\n",
       " 'check',\n",
       " 'schedule',\n",
       " 'amp',\n",
       " 'dm',\n",
       " '😀',\n",
       " 'sweet',\n",
       " 'day',\n",
       " 'coming',\n",
       " 'multi',\n",
       " 'faith',\n",
       " 'hotel',\n",
       " 'room',\n",
       " '😀',\n",
       " 'loved',\n",
       " 'album',\n",
       " 'always',\n",
       " 'thought',\n",
       " 'little',\n",
       " 'miss',\n",
       " 'cant',\n",
       " 'wrong',\n",
       " 'thanks',\n",
       " 'great',\n",
       " 'review',\n",
       " 'love',\n",
       " 'show',\n",
       " 'radionz',\n",
       " 'reliable',\n",
       " 'cheer',\n",
       " 'bro',\n",
       " 'cry',\n",
       " 'havoc',\n",
       " 'let',\n",
       " 'slip',\n",
       " 'dog',\n",
       " 'war',\n",
       " 'await',\n",
       " 'barrage',\n",
       " 'tweet',\n",
       " 'fullas',\n",
       " 'tomorrow',\n",
       " 'way',\n",
       " 'welly',\n",
       " 'work',\n",
       " '😀',\n",
       " 'ok',\n",
       " 'stand',\n",
       " 'lot',\n",
       " 'tweet',\n",
       " 'paeds',\n",
       " 'etc',\n",
       " 'diluting',\n",
       " 'expectation',\n",
       " 'plenty',\n",
       " 'time',\n",
       " 'get',\n",
       " 'whipped',\n",
       " 'frenzy',\n",
       " 'surely',\n",
       " 'thats',\n",
       " 'prime',\n",
       " 'time',\n",
       " 'wolfpack',\n",
       " 'radio',\n",
       " 'silence',\n",
       " 'smaccdub',\n",
       " 'disturbing',\n",
       " 'woody',\n",
       " 'cool',\n",
       " 'somehow']"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_docs[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### FEATURE EXTRACTION"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### TFID VECTORIZER"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def dummy_fun(doc):\n",
    "    return doc\n",
    "\n",
    "\n",
    "vectorizer = TfidfVectorizer(\n",
    "    analyzer='word',\n",
    "    input='content',\n",
    "    token_pattern=r'[^\\s]+',\n",
    "    min_df = 15,\n",
    "    max_df=0.9,\n",
    "    tokenizer=dummy_fun,\n",
    "    preprocessor=dummy_fun) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train_tfidf=vectorizer.fit_transform(train_docs)\n",
    "y_train_tfidf=np.asarray(train_labels)\n",
    "\n",
    "x_valid_tfidf=vectorizer.transform(validation_docs)\n",
    "y_valid_tfidf=np.asarray(validation_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['aa',\n",
       " 'aaron',\n",
       " 'ab',\n",
       " 'abandon',\n",
       " 'abandoned',\n",
       " 'abbott',\n",
       " 'abc',\n",
       " 'aberdeen',\n",
       " 'ability',\n",
       " 'able',\n",
       " 'aboard',\n",
       " 'aboriginal',\n",
       " 'abortion',\n",
       " 'abroad',\n",
       " 'absence',\n",
       " 'absolute',\n",
       " 'absolutely',\n",
       " 'abstract',\n",
       " 'absurd',\n",
       " 'abt',\n",
       " 'abuse',\n",
       " 'abused',\n",
       " 'abuser',\n",
       " 'abusive',\n",
       " 'ac',\n",
       " 'aca',\n",
       " 'academic',\n",
       " 'academy',\n",
       " 'acc',\n",
       " 'accent',\n",
       " 'accept',\n",
       " 'acceptable',\n",
       " 'acceptance',\n",
       " 'accepted',\n",
       " 'accepting',\n",
       " 'access',\n",
       " 'accessibility',\n",
       " 'accessible',\n",
       " 'accessory',\n",
       " 'accident',\n",
       " 'accidental',\n",
       " 'accidentally',\n",
       " 'accommodation',\n",
       " 'accomplish',\n",
       " 'accomplished',\n",
       " 'accomplishment',\n",
       " 'according',\n",
       " 'account',\n",
       " 'accountability',\n",
       " 'accountable',\n",
       " 'acct',\n",
       " 'accuracy',\n",
       " 'accurate',\n",
       " 'accurately',\n",
       " 'accusation',\n",
       " 'accused',\n",
       " 'ace',\n",
       " 'ache',\n",
       " 'achieve',\n",
       " 'achieved',\n",
       " 'achievement',\n",
       " 'achieving',\n",
       " 'acid',\n",
       " 'acknowledge',\n",
       " 'aclu',\n",
       " 'acoustic',\n",
       " 'acquired',\n",
       " 'acquisition',\n",
       " 'across',\n",
       " 'act',\n",
       " 'acting',\n",
       " 'action',\n",
       " 'active',\n",
       " 'actively',\n",
       " 'activism',\n",
       " 'activist',\n",
       " 'activity',\n",
       " 'actor',\n",
       " 'actress',\n",
       " 'actual',\n",
       " 'actually',\n",
       " 'ad',\n",
       " 'adam',\n",
       " 'adapt',\n",
       " 'add',\n",
       " 'added',\n",
       " 'addict',\n",
       " 'addicted',\n",
       " 'addiction',\n",
       " 'addictive',\n",
       " 'adding',\n",
       " 'addition',\n",
       " 'additional',\n",
       " 'address',\n",
       " 'addressed',\n",
       " 'addressing',\n",
       " 'adelaide',\n",
       " 'adidas',\n",
       " 'admin',\n",
       " 'administration',\n",
       " 'admiration',\n",
       " 'admire',\n",
       " 'admired',\n",
       " 'admit',\n",
       " 'admits',\n",
       " 'admitted',\n",
       " 'admitting',\n",
       " 'adopt',\n",
       " 'adopted',\n",
       " 'adoption',\n",
       " 'adorable',\n",
       " 'adore',\n",
       " 'adult',\n",
       " 'advance',\n",
       " 'advanced',\n",
       " 'advantage',\n",
       " 'advent',\n",
       " 'adventure',\n",
       " 'advert',\n",
       " 'advertised',\n",
       " 'advertising',\n",
       " 'advice',\n",
       " 'advise',\n",
       " 'advised',\n",
       " 'adviser',\n",
       " 'advisor',\n",
       " 'advisory',\n",
       " 'advocacy',\n",
       " 'advocate',\n",
       " 'advocating',\n",
       " 'ae',\n",
       " 'aesthetic',\n",
       " 'af',\n",
       " 'afc',\n",
       " 'affair',\n",
       " 'affect',\n",
       " 'affected',\n",
       " 'affection',\n",
       " 'affleck',\n",
       " 'afford',\n",
       " 'affordability',\n",
       " 'affordable',\n",
       " 'afghan',\n",
       " 'afghanistan',\n",
       " 'afl',\n",
       " 'afraid',\n",
       " 'africa',\n",
       " 'african',\n",
       " 'aftermath',\n",
       " 'afternoon',\n",
       " 'aftershock',\n",
       " 'afterwards',\n",
       " 'ag',\n",
       " 'age',\n",
       " 'aged',\n",
       " 'agency',\n",
       " 'agenda',\n",
       " 'agent',\n",
       " 'aggressive',\n",
       " 'agile',\n",
       " 'ago',\n",
       " 'agree',\n",
       " 'agreed',\n",
       " 'agreeing',\n",
       " 'agreement',\n",
       " 'agrees',\n",
       " 'agriculture',\n",
       " 'ah',\n",
       " 'aha',\n",
       " 'ahaha',\n",
       " 'ahead',\n",
       " 'ahem',\n",
       " 'ahh',\n",
       " 'ahhh',\n",
       " 'ahhhh',\n",
       " 'ahhhhh',\n",
       " 'ai',\n",
       " 'aid',\n",
       " 'aide',\n",
       " 'aim',\n",
       " 'aimed',\n",
       " 'aiming',\n",
       " 'aint',\n",
       " 'air',\n",
       " 'airbnb',\n",
       " 'aircraft',\n",
       " 'aired',\n",
       " 'airing',\n",
       " 'airline',\n",
       " 'airplane',\n",
       " 'airport',\n",
       " 'airtime',\n",
       " 'aisle',\n",
       " 'aj',\n",
       " 'aka',\n",
       " 'akl',\n",
       " 'al',\n",
       " 'ala',\n",
       " 'alabama',\n",
       " 'alan',\n",
       " 'alarm',\n",
       " 'alarming',\n",
       " 'alaska',\n",
       " 'albeit',\n",
       " 'albert',\n",
       " 'alberta',\n",
       " 'album',\n",
       " 'alcohol',\n",
       " 'ale',\n",
       " 'aleppo',\n",
       " 'alert',\n",
       " 'alex',\n",
       " 'alexa',\n",
       " 'alexander',\n",
       " 'algorithm',\n",
       " 'ali',\n",
       " 'alice',\n",
       " 'alien',\n",
       " 'alike',\n",
       " 'alive',\n",
       " 'allegation',\n",
       " 'alleged',\n",
       " 'allegedly',\n",
       " 'allen',\n",
       " 'allergic',\n",
       " 'alliance',\n",
       " 'allow',\n",
       " 'allowance',\n",
       " 'allowed',\n",
       " 'allowing',\n",
       " 'allows',\n",
       " 'alls',\n",
       " 'alltime',\n",
       " 'ally',\n",
       " 'alma',\n",
       " 'almond',\n",
       " 'almost',\n",
       " 'alone',\n",
       " 'along',\n",
       " 'alongside',\n",
       " 'alot',\n",
       " 'aloud',\n",
       " 'alp',\n",
       " 'alpha',\n",
       " 'already',\n",
       " 'alright',\n",
       " 'also',\n",
       " 'alt',\n",
       " 'alter',\n",
       " 'alternate',\n",
       " 'alternative',\n",
       " 'alternativefact',\n",
       " 'alternativefacts',\n",
       " 'although',\n",
       " 'altogether',\n",
       " 'altright',\n",
       " 'alumnus',\n",
       " 'always',\n",
       " 'alzheimers',\n",
       " 'amanda',\n",
       " 'amateur',\n",
       " 'amaze',\n",
       " 'amazed',\n",
       " 'amazing',\n",
       " 'amazingly',\n",
       " 'amazon',\n",
       " 'ambassador',\n",
       " 'amber',\n",
       " 'ambition',\n",
       " 'ambitious',\n",
       " 'ambulance',\n",
       " 'amen',\n",
       " 'amendment',\n",
       " 'america',\n",
       " 'american',\n",
       " 'amid',\n",
       " 'amidst',\n",
       " 'among',\n",
       " 'amongst',\n",
       " 'amount',\n",
       " 'amp',\n",
       " 'amsterdam',\n",
       " 'amused',\n",
       " 'amusing',\n",
       " 'amy',\n",
       " 'analogy',\n",
       " 'analysis',\n",
       " 'analyst',\n",
       " 'analytics',\n",
       " 'anarchist',\n",
       " 'ancestor',\n",
       " 'anchor',\n",
       " 'ancient',\n",
       " 'anderson',\n",
       " 'andor',\n",
       " 'andrew',\n",
       " 'android',\n",
       " 'andy',\n",
       " 'angel',\n",
       " 'angela',\n",
       " 'angeles',\n",
       " 'anger',\n",
       " 'angle',\n",
       " 'angry',\n",
       " 'animal',\n",
       " 'animated',\n",
       " 'animation',\n",
       " 'ankle',\n",
       " 'ann',\n",
       " 'anna',\n",
       " 'anne',\n",
       " 'anniversary',\n",
       " 'announce',\n",
       " 'announced',\n",
       " 'announcement',\n",
       " 'announces',\n",
       " 'announcing',\n",
       " 'annoy',\n",
       " 'annoyed',\n",
       " 'annoying',\n",
       " 'annoys',\n",
       " 'annual',\n",
       " 'anonymous',\n",
       " 'another',\n",
       " 'answer',\n",
       " 'answered',\n",
       " 'answering',\n",
       " 'ant',\n",
       " 'antarctica',\n",
       " 'anthem',\n",
       " 'anthony',\n",
       " 'anti',\n",
       " 'antiabortion',\n",
       " 'antibiotic',\n",
       " 'antic',\n",
       " 'anticipation',\n",
       " 'antitrump',\n",
       " 'antonio',\n",
       " 'anxiety',\n",
       " 'anxious',\n",
       " 'anybody',\n",
       " 'anymore',\n",
       " 'anyone',\n",
       " 'anyones',\n",
       " 'anything',\n",
       " 'anytime',\n",
       " 'anyway',\n",
       " 'anyways',\n",
       " 'anywhere',\n",
       " 'anz',\n",
       " 'aotearoa',\n",
       " 'ap',\n",
       " 'apart',\n",
       " 'apartment',\n",
       " 'api',\n",
       " 'apocalypse',\n",
       " 'apologise',\n",
       " 'apologize',\n",
       " 'apology',\n",
       " 'app',\n",
       " 'appalled',\n",
       " 'appalling',\n",
       " 'apparent',\n",
       " 'apparently',\n",
       " 'appeal',\n",
       " 'appealing',\n",
       " 'appear',\n",
       " 'appearance',\n",
       " 'appeared',\n",
       " 'appearing',\n",
       " 'appears',\n",
       " 'appetite',\n",
       " 'applaud',\n",
       " 'applause',\n",
       " 'apple',\n",
       " 'application',\n",
       " 'applied',\n",
       " 'applies',\n",
       " 'apply',\n",
       " 'applying',\n",
       " 'appoint',\n",
       " 'appointed',\n",
       " 'appointment',\n",
       " 'appoints',\n",
       " 'appreciate',\n",
       " 'appreciated',\n",
       " 'appreciation',\n",
       " 'apprentice',\n",
       " 'approach',\n",
       " 'approaching',\n",
       " 'appropriate',\n",
       " 'approval',\n",
       " 'approve',\n",
       " 'approved',\n",
       " 'approx',\n",
       " 'apps',\n",
       " 'april',\n",
       " 'apt',\n",
       " 'aquarium',\n",
       " 'ar',\n",
       " 'arab',\n",
       " 'arabia',\n",
       " 'arabic',\n",
       " 'arbitrary',\n",
       " 'arch',\n",
       " 'architect',\n",
       " 'architecture',\n",
       " 'archive',\n",
       " 'arctic',\n",
       " 'area',\n",
       " 'arena',\n",
       " 'arent',\n",
       " 'argentina',\n",
       " 'argh',\n",
       " 'arguably',\n",
       " 'argue',\n",
       " 'arguing',\n",
       " 'argument',\n",
       " 'arizona',\n",
       " 'arm',\n",
       " 'armed',\n",
       " 'armstrong',\n",
       " 'army',\n",
       " 'arnold',\n",
       " 'around',\n",
       " 'arrange',\n",
       " 'arranged',\n",
       " 'arrangement',\n",
       " 'arrest',\n",
       " 'arrested',\n",
       " 'arrival',\n",
       " 'arrive',\n",
       " 'arrived',\n",
       " 'arrives',\n",
       " 'arriving',\n",
       " 'arrogant',\n",
       " 'arrow',\n",
       " 'arse',\n",
       " 'arsed',\n",
       " 'arsehole',\n",
       " 'arsenal',\n",
       " 'art',\n",
       " 'arthur',\n",
       " 'article',\n",
       " 'articulate',\n",
       " 'artificial',\n",
       " 'artist',\n",
       " 'artistic',\n",
       " 'artwork',\n",
       " 'arvo',\n",
       " 'as',\n",
       " 'asap',\n",
       " 'ash',\n",
       " 'ashamed',\n",
       " 'ashley',\n",
       " 'asia',\n",
       " 'asian',\n",
       " 'aside',\n",
       " 'ask',\n",
       " 'asked',\n",
       " 'asking',\n",
       " 'asks',\n",
       " 'asleep',\n",
       " 'aspect',\n",
       " 'aspiration',\n",
       " 'aspire',\n",
       " 'ass',\n",
       " 'assange',\n",
       " 'assault',\n",
       " 'assessment',\n",
       " 'asset',\n",
       " 'asshole',\n",
       " 'assignment',\n",
       " 'assist',\n",
       " 'assistance',\n",
       " 'assistant',\n",
       " 'assisted',\n",
       " 'associate',\n",
       " 'associated',\n",
       " 'association',\n",
       " 'assume',\n",
       " 'assumed',\n",
       " 'assuming',\n",
       " 'assumption',\n",
       " 'astonishing',\n",
       " 'astounding',\n",
       " 'astronaut',\n",
       " 'aswell',\n",
       " 'asylum',\n",
       " 'ate',\n",
       " 'atheist',\n",
       " 'athlete',\n",
       " 'atlanta',\n",
       " 'atlantic',\n",
       " 'atm',\n",
       " 'atmosphere',\n",
       " 'atrocious',\n",
       " 'atrocity',\n",
       " 'attached',\n",
       " 'attack',\n",
       " 'attacked',\n",
       " 'attacker',\n",
       " 'attacking',\n",
       " 'attempt',\n",
       " 'attempted',\n",
       " 'attempting',\n",
       " 'attend',\n",
       " 'attendance',\n",
       " 'attended',\n",
       " 'attendee',\n",
       " 'attending',\n",
       " 'attention',\n",
       " 'attic',\n",
       " 'attitude',\n",
       " 'attorney',\n",
       " 'attract',\n",
       " 'attracted',\n",
       " 'attraction',\n",
       " 'attractive',\n",
       " 'au',\n",
       " 'auckland',\n",
       " 'auction',\n",
       " 'audience',\n",
       " 'audio',\n",
       " 'audit',\n",
       " 'audition',\n",
       " 'aug',\n",
       " 'august',\n",
       " 'aunt',\n",
       " 'auntie',\n",
       " 'aunty',\n",
       " 'ausopen',\n",
       " 'auspol',\n",
       " 'aussie',\n",
       " 'aust',\n",
       " 'austin',\n",
       " 'australia',\n",
       " 'australiaday',\n",
       " 'australian',\n",
       " 'austria',\n",
       " 'authentic',\n",
       " 'author',\n",
       " 'authoritarian',\n",
       " 'authority',\n",
       " 'autism',\n",
       " 'auto',\n",
       " 'autocorrect',\n",
       " 'automated',\n",
       " 'automatic',\n",
       " 'automatically',\n",
       " 'automation',\n",
       " 'autumn',\n",
       " 'av',\n",
       " 'avail',\n",
       " 'availability',\n",
       " 'available',\n",
       " 'avalanche',\n",
       " 'ave',\n",
       " 'avenue',\n",
       " 'average',\n",
       " 'avo',\n",
       " 'avocado',\n",
       " 'avoid',\n",
       " 'avoided',\n",
       " 'avoiding',\n",
       " 'aw',\n",
       " 'await',\n",
       " 'awaiting',\n",
       " 'awaits',\n",
       " 'awake',\n",
       " 'award',\n",
       " 'awarded',\n",
       " 'aware',\n",
       " 'awareness',\n",
       " 'away',\n",
       " 'awe',\n",
       " 'awesome',\n",
       " 'awesomeness',\n",
       " 'awful',\n",
       " 'awfully',\n",
       " 'awh',\n",
       " 'awhile',\n",
       " 'awkward',\n",
       " 'aww',\n",
       " 'awww',\n",
       " 'awwww',\n",
       " 'aye',\n",
       " 'ba',\n",
       " 'babe',\n",
       " 'baby',\n",
       " 'bachelor',\n",
       " 'back',\n",
       " 'backbone',\n",
       " 'backed',\n",
       " 'background',\n",
       " 'backing',\n",
       " 'backlash',\n",
       " 'backpack',\n",
       " 'backup',\n",
       " 'backwards',\n",
       " 'backyard',\n",
       " 'bacon',\n",
       " 'bad',\n",
       " 'badass',\n",
       " 'badge',\n",
       " 'badger',\n",
       " 'badly',\n",
       " 'bae',\n",
       " 'baffle',\n",
       " 'baffled',\n",
       " 'bag',\n",
       " 'baggage',\n",
       " 'bail',\n",
       " 'bailey',\n",
       " 'baird',\n",
       " 'bait',\n",
       " 'bake',\n",
       " 'baked',\n",
       " 'baker',\n",
       " 'bakery',\n",
       " 'baking',\n",
       " 'balance',\n",
       " 'balanced',\n",
       " 'balancing',\n",
       " 'balcony',\n",
       " 'bald',\n",
       " 'baldwin',\n",
       " 'bali',\n",
       " 'ball',\n",
       " 'balloon',\n",
       " 'ballot',\n",
       " 'bam',\n",
       " 'ban',\n",
       " 'banana',\n",
       " 'band',\n",
       " 'bandwagon',\n",
       " 'bang',\n",
       " 'banger',\n",
       " 'banging',\n",
       " 'bank',\n",
       " 'banker',\n",
       " 'banking',\n",
       " 'bankrupt',\n",
       " 'banned',\n",
       " 'banner',\n",
       " 'banning',\n",
       " 'bannon',\n",
       " 'bannons',\n",
       " 'banter',\n",
       " 'bar',\n",
       " 'barack',\n",
       " 'barbara',\n",
       " 'barber',\n",
       " 'barca',\n",
       " 'barcelona',\n",
       " 'bare',\n",
       " 'barely',\n",
       " 'bargain',\n",
       " 'barnaby',\n",
       " 'barred',\n",
       " 'barrel',\n",
       " 'barrier',\n",
       " 'barron',\n",
       " 'barry',\n",
       " 'base',\n",
       " 'baseball',\n",
       " 'based',\n",
       " 'basement',\n",
       " 'bash',\n",
       " 'bashing',\n",
       " 'basic',\n",
       " 'basically',\n",
       " 'basil',\n",
       " 'basin',\n",
       " 'basis',\n",
       " 'basket',\n",
       " 'basketball',\n",
       " 'bass',\n",
       " 'bastard',\n",
       " 'bat',\n",
       " 'batch',\n",
       " 'bath',\n",
       " 'bathroom',\n",
       " 'batman',\n",
       " 'batsman',\n",
       " 'battery',\n",
       " 'batting',\n",
       " 'battle',\n",
       " 'battling',\n",
       " 'bay',\n",
       " 'bb',\n",
       " 'bbc',\n",
       " 'bbq',\n",
       " 'bc',\n",
       " 'bcs',\n",
       " 'bcstorm',\n",
       " 'bday',\n",
       " 'beach',\n",
       " 'bean',\n",
       " 'bear',\n",
       " 'beard',\n",
       " 'beast',\n",
       " 'beat',\n",
       " 'beaten',\n",
       " 'beating',\n",
       " 'beatles',\n",
       " 'beaut',\n",
       " 'beautiful',\n",
       " 'beautifully',\n",
       " 'beauty',\n",
       " 'became',\n",
       " 'become',\n",
       " 'becomes',\n",
       " 'becoming',\n",
       " 'bed',\n",
       " 'bedroom',\n",
       " 'bedtime',\n",
       " 'bee',\n",
       " 'beef',\n",
       " 'beer',\n",
       " 'beg',\n",
       " 'began',\n",
       " 'begging',\n",
       " 'begin',\n",
       " 'beginner',\n",
       " 'beginning',\n",
       " 'begun',\n",
       " 'behalf',\n",
       " 'behave',\n",
       " 'behaving',\n",
       " 'behavior',\n",
       " 'behaviour',\n",
       " 'behind',\n",
       " 'behold',\n",
       " 'beijing',\n",
       " 'being',\n",
       " 'belated',\n",
       " 'belfast',\n",
       " 'belgium',\n",
       " 'belief',\n",
       " 'believe',\n",
       " 'believed',\n",
       " 'believer',\n",
       " 'believing',\n",
       " 'bell',\n",
       " 'bellletstalk',\n",
       " 'belly',\n",
       " 'belong',\n",
       " 'belongs',\n",
       " 'beloved',\n",
       " 'belt',\n",
       " 'ben',\n",
       " 'bench',\n",
       " 'bend',\n",
       " 'beneath',\n",
       " 'beneficial',\n",
       " 'benefit',\n",
       " 'benjamin',\n",
       " 'bennett',\n",
       " 'bent',\n",
       " 'berkeley',\n",
       " 'berlin',\n",
       " 'bernard',\n",
       " 'bernie',\n",
       " 'berry',\n",
       " 'beside',\n",
       " 'besides',\n",
       " 'best',\n",
       " 'bestie',\n",
       " 'bet',\n",
       " 'beta',\n",
       " 'betrayal',\n",
       " 'betrayed',\n",
       " 'betsy',\n",
       " 'better',\n",
       " 'betting',\n",
       " 'betty',\n",
       " 'beverage',\n",
       " 'beware',\n",
       " 'beyonce',\n",
       " 'beyoncé',\n",
       " 'beyond',\n",
       " 'bf',\n",
       " 'bff',\n",
       " 'bi',\n",
       " 'bianca',\n",
       " 'bias',\n",
       " 'biased',\n",
       " 'bible',\n",
       " 'bicycle',\n",
       " 'bid',\n",
       " 'biden',\n",
       " 'bieber',\n",
       " 'big',\n",
       " 'bigger',\n",
       " 'biggest',\n",
       " 'bigly',\n",
       " 'bigot',\n",
       " 'bigoted',\n",
       " 'bigotry',\n",
       " 'bike',\n",
       " 'bikini',\n",
       " 'bill',\n",
       " 'billion',\n",
       " 'billionaire',\n",
       " 'billy',\n",
       " 'bin',\n",
       " 'binge',\n",
       " 'bingo',\n",
       " 'bio',\n",
       " 'bipartisan',\n",
       " 'bird',\n",
       " 'birmingham',\n",
       " 'birth',\n",
       " 'birthday',\n",
       " 'biscuit',\n",
       " 'bishop',\n",
       " 'bit',\n",
       " 'bitch',\n",
       " 'bite',\n",
       " 'biting',\n",
       " 'bitter',\n",
       " 'bittersweet',\n",
       " 'biz',\n",
       " 'bizarre',\n",
       " 'black',\n",
       " 'blackberry',\n",
       " 'blackhistorymonth',\n",
       " 'blackout',\n",
       " 'blade',\n",
       " 'blair',\n",
       " 'blake',\n",
       " 'blame',\n",
       " 'blamed',\n",
       " 'blaming',\n",
       " 'blank',\n",
       " 'blanket',\n",
       " 'blast',\n",
       " 'blatant',\n",
       " 'blatantly',\n",
       " 'bleach',\n",
       " 'bleeding',\n",
       " 'blend',\n",
       " 'bless',\n",
       " 'blessed',\n",
       " 'blessing',\n",
       " 'blew',\n",
       " 'blimey',\n",
       " 'blind',\n",
       " 'blindly',\n",
       " 'blink',\n",
       " 'bliss',\n",
       " 'blizzard',\n",
       " 'block',\n",
       " 'blockchain',\n",
       " 'blocked',\n",
       " 'blocking',\n",
       " 'blog',\n",
       " 'blogger',\n",
       " 'blogging',\n",
       " 'bloke',\n",
       " 'blonde',\n",
       " 'blood',\n",
       " 'bloody',\n",
       " 'bloom',\n",
       " 'blossom',\n",
       " 'blow',\n",
       " 'blowing',\n",
       " 'blown',\n",
       " 'blue',\n",
       " 'blunt',\n",
       " 'bo',\n",
       " 'board',\n",
       " 'boarding',\n",
       " 'boat',\n",
       " 'bob',\n",
       " 'bobby',\n",
       " 'bobcat',\n",
       " 'body',\n",
       " 'boil',\n",
       " 'bold',\n",
       " 'boldly',\n",
       " 'bollock',\n",
       " 'bolt',\n",
       " 'bomb',\n",
       " 'bomber',\n",
       " 'bombing',\n",
       " 'bon',\n",
       " 'bond',\n",
       " 'bondi',\n",
       " 'bone',\n",
       " 'bonus',\n",
       " 'boo',\n",
       " 'boob',\n",
       " 'book',\n",
       " 'booked',\n",
       " 'bookie',\n",
       " 'booking',\n",
       " 'bookshop',\n",
       " 'bookstore',\n",
       " 'boom',\n",
       " 'boomer',\n",
       " 'boost',\n",
       " 'boot',\n",
       " 'booth',\n",
       " 'booty',\n",
       " 'booze',\n",
       " 'border',\n",
       " 'bore',\n",
       " 'bored',\n",
       " 'boring',\n",
       " 'boris',\n",
       " 'born',\n",
       " 'borrow',\n",
       " 'bos',\n",
       " 'boss',\n",
       " 'boston',\n",
       " 'bot',\n",
       " 'botanic',\n",
       " 'bother',\n",
       " 'bothered',\n",
       " 'bottle',\n",
       " 'bottled',\n",
       " 'bottom',\n",
       " 'bought',\n",
       " 'bounce',\n",
       " 'bouncing',\n",
       " 'bound',\n",
       " 'boundary',\n",
       " 'bournemouth',\n",
       " 'bout',\n",
       " 'bow',\n",
       " 'bowie',\n",
       " 'bowl',\n",
       " 'bowling',\n",
       " 'box',\n",
       " 'boxer',\n",
       " 'boxing',\n",
       " 'boy',\n",
       " 'boycott',\n",
       " 'boycotting',\n",
       " 'boyfriend',\n",
       " 'bra',\n",
       " 'brace',\n",
       " 'bracelet',\n",
       " 'bracket',\n",
       " 'brad',\n",
       " 'bradley',\n",
       " 'brady',\n",
       " 'brag',\n",
       " 'brain',\n",
       " 'branch',\n",
       " 'brand',\n",
       " 'branding',\n",
       " 'brave',\n",
       " 'bravo',\n",
       " 'brazil',\n",
       " 'brazilian',\n",
       " 'brb',\n",
       " 'breach',\n",
       " 'bread',\n",
       " 'break',\n",
       " 'breakdown',\n",
       " 'breakfast',\n",
       " 'breaking',\n",
       " 'breakthrough',\n",
       " 'breast',\n",
       " 'breath',\n",
       " 'breathe',\n",
       " 'breathing',\n",
       " 'breathtaking',\n",
       " 'breed',\n",
       " 'breeze',\n",
       " 'breitbart',\n",
       " 'brendan',\n",
       " 'brett',\n",
       " 'brew',\n",
       " 'brewery',\n",
       " 'brewing',\n",
       " 'brexit',\n",
       " 'brian',\n",
       " 'brick',\n",
       " 'bride',\n",
       " 'bridge',\n",
       " 'bridget',\n",
       " 'brief',\n",
       " 'briefing',\n",
       " 'briefly',\n",
       " 'bright',\n",
       " 'brighten',\n",
       " 'brighter',\n",
       " 'brighton',\n",
       " 'brill',\n",
       " 'brilliance',\n",
       " 'brilliant',\n",
       " 'brilliantly',\n",
       " 'bring',\n",
       " 'bringing',\n",
       " 'brings',\n",
       " 'brisbane',\n",
       " ...]"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vectorizer.get_feature_names()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['amp', 'bought', 'smell', 'whisky', 'ive', 'daughter', 'got',\n",
       "       'spotify', 'locked', 'like', 'wash', 'accidentally', 'feel',\n",
       "       'crack', 'trying', 'ending', 'direction', 'much', 'putin',\n",
       "       'computer', 'room', 'checked', 'im', 'hahaha', 'shoe', 'worth',\n",
       "       'eh', 'russian', 'stuff', 'know', 'something', 'good', 'album',\n",
       "       'leg', 'wife', 'sad', 'sell', 'great', 'quality', 'terrible',\n",
       "       'wellington', 'site', 'sort', 'let', 'go', 'following', 'nz',\n",
       "       'cursed', 'rear', 'amanda', 'crashing', 'accidental', 'glowing',\n",
       "       'blue', 'dont', 'infinite', 'clicked', 'suited', 'rug', 'dumbest',\n",
       "       'usage', 'who', 'thee', 'yet', 'filthy', 'come', 'day', 'change',\n",
       "       'correlation', 'trusting', 'parrot', 'away', 'googling', 'opener',\n",
       "       'sting', 'thats', 'chrome', 'folder', 'smack', 'pirate', 'maximum',\n",
       "       'donkey', 'oddly', 'passive', 'daylight', 'oxford', 'handing',\n",
       "       'luggage', 'end', 'stitch', 'paranoid', 'rome', 'tshirts', 'dream',\n",
       "       'listening', 'disappeared', 'smug', 'behind', 'living',\n",
       "       'intervention', 'soap', 'guilt', 'geez', 'apologise', 'condolence',\n",
       "       'browser', 'chch', 'thing', 'lamp', 'man', 'scot', 'relaxing',\n",
       "       'increasingly', 'handsome', 'tick', 'song', 'carrying', 'absurd',\n",
       "       'halfway', 'hit', 'relate', 'latte', 'christchurch', 'rabbit',\n",
       "       'commentator', 'temperature', 'jazz', 'pit', 'danny', 'inch',\n",
       "       'people', 'ipad', 'additional', 'therefore', 'infographic',\n",
       "       'monitor', 'vote', 'takeaway', 'make', 'bothered', 'combined',\n",
       "       'cave', 'portrait', 'list', 'keith', 'proposed', 'uh', 'billy',\n",
       "       'loop', 'flu', 'wilson', 'equivalent', 'upper', 'presence',\n",
       "       'tshirt', 'curry', 'attacking', 'commit', 'moment', 'cent',\n",
       "       'stole', 'worrying', 'screw', 'complaint', 'disgrace', 'bucket',\n",
       "       'welly', 'rant', 'soldier', 'mar', 'cable', 'throughout',\n",
       "       'advertising', 'fridge', 'seems', 'arrival', 'ignored', 'went',\n",
       "       'money', 'used', 'booking', 'steal', 'younger', 'spare', 'jam',\n",
       "       'pretend', 'imo', 'convinced', 'recovery', 'essay', 'neil',\n",
       "       'expecting', 'mac', 'device', 'faster', 'wish', 'pant', 'odds',\n",
       "       'whilst', 'butter', 'eg', 'raised', 'specific', 'anxiety', 'arse',\n",
       "       'working', 'teen', 'huh', 'fixed', 'kiss', 'stephen', 'lane',\n",
       "       'van', 'scale', 'previous', 'true', 'ate', 'forgotten', 'tony',\n",
       "       'seen', 'mixed', 'sam', 'wrong', 'family', 'blessed', 'opened',\n",
       "       'immediately', 'direct', 'troll', 'status', 'going', 'whats',\n",
       "       'pretty', 'jack', 'try', 'goodness', 'moon', 'budget', 'cant',\n",
       "       'stock', 'solid', 'button', 'realised', 'understanding', 'shower',\n",
       "       'place', 'look', 'mirror', 'sorted', 'delicious', 'proper',\n",
       "       'kitchen', 'bit', 'iphone', 'call', 'bottom', 'hole', 'back',\n",
       "       'mobile', 'flat', 'werent', 'kept', 'saving', 'bay', 'said',\n",
       "       'failed', 'tonight', 'fat', 'think', 'sat', 'flying', 'officially',\n",
       "       'simply', 'suit', 'turning', 'father', 'audience', 'bos', 'bitch',\n",
       "       'taste', 'tiny', 'six', 'old', 'drug', 'traffic', 'quiet', 'see',\n",
       "       'notice', 'bad', 'unfortunately', 'read', 'someone', 'dr',\n",
       "       'bigger', 'rise', 'year', 'somewhere', 'network', 'wrote', 'done',\n",
       "       'home', 'code', 'died', 'honest', 'mess', 'theyll', 'paying',\n",
       "       'radio', 'caught', 'common', 'shut', 'throw', 'greatest', 'credit',\n",
       "       'table', 'powerful', 'keep', 'forever', 'yay', 'theyve', 'slow',\n",
       "       'letter', 'warm', 'also', 'wondering', 'process', 'co', 'stage',\n",
       "       'seat', 'beer', 'written', 'thought', 'show', 'hearing',\n",
       "       'currently', 'tea', 'daily', 'mention', 'youd', 'release', 'data',\n",
       "       'cup', 'mum', 'wake', 'gotta', 'award', 'pas', 'gift', 'ha',\n",
       "       'youre', 'son', 'nation', 'laugh', 'lunch', 'straight', 'wear',\n",
       "       'die', 'eating', 'parent', 'price', 'next', 'internet', 'asking',\n",
       "       'yep', 'bar', 'inside', 'serious', 'app', 'secret', 'calling',\n",
       "       'played', 'sweet', 'loving', 'body', 'facebook', 'paid', 'train',\n",
       "       'ah', 'bill', 'thanks', 'sick', 'hopefully', 'dad', 'update',\n",
       "       'happening', 'bus', 'joke', 'voice', 'lady', 'could', 'knew',\n",
       "       'visit', 'message', 'wall', 'drink', 'save', 'choice', 'become',\n",
       "       'card', 'coffee', 'tried', 'really', 'take', 'fine', 'forget',\n",
       "       'level', 'eat', 'young', 'hi', 'rather', 'haha', 'say', 'listen',\n",
       "       'cause', 'age', 'water', 'poor', 'way', 'worst', 'happened',\n",
       "       'picture', 'wanted', 'instead', 'love', 'glad', 'sunday', 'music',\n",
       "       'eye', 'stand', 'office', 'article', 'past', 'black', 'definitely',\n",
       "       'car', 'exactly', 'reading', 'shes', 'youve', 'wonder', 'number',\n",
       "       'tv', 'side', 'different', 'future', 'business', 'order', 'heard',\n",
       "       'power', 'turn', 'awesome', 'heart', 'else', 'interesting',\n",
       "       'excited', 'new', 'wasnt', 'mind', 'taking', 'miss', 'issue',\n",
       "       'saying', 'hand', 'thinking', 'fact', 'found', 'ago', 'least',\n",
       "       'fun', 'yeah', 'full', 'month', 'christmas', 'wont', 'check',\n",
       "       'talk', 'twitter', 'without', 'far', 'since', 'believe', 'hour',\n",
       "       'tomorrow', 'amazing', 'word', 'might', 'use', 'must', 'woman',\n",
       "       'anyone', 'hard', 'around', 'he', 'there', 'real', 'tell', 'live',\n",
       "       'morning', 'actually', 'everyone', 'put', 'wait', 'may', 'news',\n",
       "       'mean', 'little', 'ill', 'guy', 'watch', 'find', 'big', 'please',\n",
       "       'oh', 'looking', 'didnt', 'every', 'ever', 'sure', 'getting',\n",
       "       'never', 'even', 'last', 'best', 'first', 'still', 'right', 'well',\n",
       "       'would', 'today', 'need', 'feeling', 'fixing', 'fit', 'fella',\n",
       "       'donate', 'dragon', 'fitbit', 'fell', 'galaxy', 'dragged',\n",
       "       'filling', 'fitness', 'flaw', 'fifa', 'fitting', 'gallery',\n",
       "       'filled', 'gap', 'five', 'fix', 'filler', 'fixture', 'fifth',\n",
       "       'feedback', 'feeding', 'fighter', 'fig', 'fiji', 'fee', 'fight',\n",
       "       'figured', 'federernadal', 'flatmate', 'fk', 'flavor', 'flavour',\n",
       "       'fighting', 'federer', 'gala', 'figure', 'garlic', 'file', 'filed',\n",
       "       'flashback', 'flash', 'flame', 'flag', 'feed', 'fl', 'fifty',\n",
       "       'draft', 'filibuster', 'donated', 'garage', 'drag', 'fkn', 'fill',\n",
       "       'film', 'feud', 'downloading', 'finale', 'ffs', 'ferrari',\n",
       "       'finally', 'finger', 'finalist', 'fg', 'finish', 'fierce', 'fence',\n",
       "       'finished', 'final', 'fi', 'finishing', 'galway', 'ferry', 'fest',\n",
       "       'downstairs', 'gameofthrones', 'gaming', 'gardening', 'festival',\n",
       "       'finest', 'downtown', 'ff', 'festive', 'gang', 'fewer', 'finance',\n",
       "       'garden', 'finding', 'financial', 'finland', 'finn', 'gareth',\n",
       "       'female', 'filmed', 'fellow', 'garbage', 'fist', 'field',\n",
       "       'fishing', 'fever', 'fisher', 'filming', 'fish', 'firstdatesirl',\n",
       "       'filmmaker', 'felt', 'filter', 'fiction', 'feminist', 'downside',\n",
       "       'garland', 'firmly', 'fibre', 'dozen', 'firm', 'firing', 'fin',\n",
       "       'feminism', 'firework', 'fired', 'fire', 'fiona', 'fintech',\n",
       "       'game', 'gadget', 'flawed', 'frankly', 'framing', 'fran', 'france',\n",
       "       'dope', 'franchise', 'francis', 'francisco', 'frank', 'funeral',\n",
       "       'frankie', 'fundraising', 'fundraiser', 'door', 'downloaded',\n",
       "       'fraser', 'fraud', 'freak', 'freaked', 'freakin', 'freaking',\n",
       "       'funding', 'freaky', 'fred', 'freddie', 'frederick', 'doomsday',\n",
       "       'framework', 'framed', 'frame', 'fragile', 'funny', 'fortnight',\n",
       "       'fortunate', 'fortune', 'forum', 'forward', 'funniest', 'fossil',\n",
       "       'foster', 'fought', 'foul', 'funnier', 'foundation', 'dossier',\n",
       "       'founded', 'founder', 'funky', 'dose', 'fountain', 'four',\n",
       "       'fourth', 'funk', 'fox', 'fr', 'fraction', 'free', 'funded',\n",
       "       'freedom', 'frog', 'frontier', 'functioning', 'frontline', 'frost',\n",
       "       'frosty', 'frozen', 'fruit', 'frustrated', 'frustrating',\n",
       "       'frustration', 'donor', 'fry', 'functional', 'ft', 'ftw',\n",
       "       'function', 'fuck', 'fucked', 'fucker', 'donna', 'fuckin',\n",
       "       'fucking', 'fudge', 'fuel', 'fully', 'front', 'donut', 'freelance',\n",
       "       'frm', 'freely', 'freeze', 'doomed', 'freezer', 'freezing',\n",
       "       'french', 'frequency', 'frequent', 'frequently', 'fresh',\n",
       "       'freshly', 'fri', 'friday', 'fundamentally', 'fridayfeeling',\n",
       "       'fried', 'friend', 'friendly', 'friendship', 'fundamental',\n",
       "       'frightening', 'fund', 'doom', 'fringe', 'friyay', 'forth', 'fort',\n",
       "       'fur', 'floor', 'floral', 'donation', 'florida', 'flow', 'flower',\n",
       "       'flowing', 'flown', 'floyd', 'gah', 'fluffy', 'fly', 'flyer',\n",
       "       'gaga', 'flynn', 'fm', 'fml', 'fo', 'downfall', 'gag', 'gaff',\n",
       "       'focus', 'down', 'focused', 'gabriel', 'dow', 'flop', 'flooding',\n",
       "       'focusing', 'gain', 'gal', 'flawless', 'fled', 'fleeing', 'fleet',\n",
       "       'flesh', 'flew', 'gaining', 'flexibility', 'flexible', 'flick',\n",
       "       'flight', 'gained', 'flint', 'flip', 'flipped', 'flipping',\n",
       "       'flirt', 'float', 'floating', 'download', 'flock', 'donating',\n",
       "       'flood', 'downhill', 'gaa', 'fog', 'formula', 'forced', 'ford',\n",
       "       'forecast', 'forehead', 'fuss', 'foreign', 'foreigner', 'forest',\n",
       "       'fury', 'doubt', 'furniture', 'doubled', 'double', 'furious',\n",
       "       'forgets', 'forgetting', 'forgive', 'forgot', 'fork', 'form',\n",
       "       'formal', 'format', 'dot', 'formation', 'formed', 'former',\n",
       "       'forcing', 'force', 'ga', 'footy', 'fold', 'fyi', 'folk', 'follow',\n",
       "       'followed', 'follower', 'douglas', 'follows', 'fomo', 'fond',\n",
       "       'font', 'food', 'foodie', 'foodporn', 'fool', 'fooled', 'foolish',\n",
       "       'fwiw', 'foot', 'footage', 'doughnut', 'football', 'fuzzy',\n",
       "       'footballer', 'footprint', 'federal', 'drunk', 'fed',\n",
       "       'embarrassment', 'embrace', 'embracing', 'emergency', 'emerges',\n",
       "       'emerging', 'emily', 'emirate', 'emission', 'emma', 'emmy', 'emo',\n",
       "       'emoji', 'emojis', 'emotion', 'driver', 'emotional', 'emotionally',\n",
       "       'empathy', 'emperor', 'emphasis', 'empire', 'employ', 'driven',\n",
       "       'employed', 'employee', 'employer', 'drive', 'employment',\n",
       "       'empower', 'embassy', 'embarrassing', 'empowerment', 'embarrassed',\n",
       "       'electionnight', 'electoral', 'electorate', 'electric',\n",
       "       'electricity', 'electronic', 'elegant', 'element', 'elementary',\n",
       "       'elephant', 'elevator', 'eleven', 'elf', 'eligible', 'elite',\n",
       "       'elizabeth', 'ella', 'ellen', 'elli', 'elliot', 'elliott', 'elon',\n",
       "       'eloquent', 'elsewhere', 'elvis', 'em', 'email', 'emailed',\n",
       "       'embarrass', 'empowering', 'empty', 'election', 'england',\n",
       "       'enhance', 'enhanced', 'enjoy', 'enjoyable', 'enjoyed', 'enjoying',\n",
       "       'enormous', 'enough', 'ensure', 'enter', 'drift', 'entered',\n",
       "       'entering', 'enterprise', 'entertained', 'dried', 'drew',\n",
       "       'entertaining', 'entertainment', 'enthusiasm', 'entire',\n",
       "       'entirely', 'dressing', 'dressed', 'entitled', 'entitlement',\n",
       "       'entity', 'dress', 'entrance', 'english', 'engineering', 'en',\n",
       "       'engineer', 'enable', 'enables', 'enabling', 'encounter',\n",
       "       'encountered', 'encourage', 'encouraged', 'encouragement'],\n",
       "      dtype='<U23')"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Top 10 Words in Feature\n",
    "feature_array = np.array(vectorizer.get_feature_names())\n",
    "tfidf_sorting = np.argsort(x_train_tfidf.toarray()).flatten()[::-1]\n",
    "\n",
    "n = 1000\n",
    "top_n = feature_array[tfidf_sorting][:n]\n",
    "top_n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### PRETRAINED GLOVE EMBEDDINGS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-62-ef6c6f685f67>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mbigram\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mPhrases\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtraining_docs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmin_count\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m40\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0midx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtraining_docs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 6\u001b[0;31m     \u001b[0;32mfor\u001b[0m \u001b[0mtoken\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mbigram\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mtraining_docs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0midx\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      7\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;34m'_'\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtoken\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m             \u001b[0;31m# Token is a bigram, add to document.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/anaconda3/lib/python3.6/site-packages/gensim/models/phrases.py\u001b[0m in \u001b[0;36m__getitem__\u001b[0;34m(self, sentence)\u001b[0m\n\u001b[1;32m    658\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    659\u001b[0m         \"\"\"\n\u001b[0;32m--> 660\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0m_sentence2token\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msentence\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    661\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    662\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/anaconda3/lib/python3.6/site-packages/gensim/models/phrases.py\u001b[0m in \u001b[0;36m_sentence2token\u001b[0;34m(phrase_class, sentence)\u001b[0m\n\u001b[1;32m    280\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    281\u001b[0m     \u001b[0mnew_s\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 282\u001b[0;31m     \u001b[0;32mfor\u001b[0m \u001b[0mwords\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mscore\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mbigrams\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    283\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mscore\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    284\u001b[0m             \u001b[0mwords\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdelimiter\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mwords\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/anaconda3/lib/python3.6/site-packages/gensim/models/phrases.py\u001b[0m in \u001b[0;36manalyze_sentence\u001b[0;34m(self, sentence, threshold, common_terms, scorer)\u001b[0m\n\u001b[1;32m    165\u001b[0m                     \u001b[0mwordb\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mword\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    166\u001b[0m                     \u001b[0mcomponents\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mchain\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 167\u001b[0;31m                     \u001b[0mscorer\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mscorer\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    168\u001b[0m                 )\n\u001b[1;32m    169\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0mscore\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0mthreshold\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/anaconda3/lib/python3.6/site-packages/gensim/models/phrases.py\u001b[0m in \u001b[0;36mscore_item\u001b[0;34m(self, worda, wordb, components, scorer)\u001b[0m\n\u001b[1;32m    123\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mbigram\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mvocab\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    124\u001b[0m                 return scorer(\n\u001b[0;32m--> 125\u001b[0;31m                     \u001b[0mworda_count\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mfloat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvocab\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mworda\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    126\u001b[0m                     \u001b[0mwordb_count\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mfloat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvocab\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mwordb\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    127\u001b[0m                     bigram_count=float(vocab[bigram]))\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "training_docs = train_docs.copy()\n",
    "#generate bigrams\n",
    "from gensim.models import Phrases\n",
    "bigram = Phrases(training_docs, min_count=40)\n",
    "for idx in range(len(training_docs)):\n",
    "    for token in bigram[training_docs[idx]]:\n",
    "        if '_' in token:\n",
    "            # Token is a bigram, add to document.\n",
    "            training_docs[idx].append(token)\n",
    "training_docs[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Pretained glove embeddings\n",
    "vocab = []\n",
    "E = {}\n",
    "with open(\"glove/glove.twitter.27B.200d.txt\", \"r\", encoding=\"utf8\") as lines:\n",
    "    for i, line in enumerate(lines):\n",
    "        l = line.split(' ')\n",
    "        if l[0].isalpha():\n",
    "            v = [float(i) for i in l[1:]]\n",
    "            E[l[0]] = np.array(v)\n",
    "            vocab.append(l[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def doc_mean(doc):\n",
    "    return np.mean(np.array(\n",
    "        [E[w.lower().strip()] for w in doc if w.lower().strip() in E]), axis=0\n",
    "            )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def transform(docs):\n",
    "        return np.array([doc_mean(doc) for doc in docs])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_docs = train_docs.copy()\n",
    "validation_data = validation_docs.copy()\n",
    "x_train = transform(training_docs)\n",
    "x_valid = transform(validation_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2480, 200)"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(620, 200)"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_valid.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train_w2vec = scale(x_train)\n",
    "x_valid_w2vec = scale(x_valid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train_w2vec =np.asarray(train_labels)\n",
    "y_valid_w2vec = np.asarray(validation_labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### LOGISTIC REGRESSION WITH TFIDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 170,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>model_name</th>\n",
       "      <th>fold_idx</th>\n",
       "      <th>accuracy</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>LogisticRegression</td>\n",
       "      <td>0</td>\n",
       "      <td>0.802419</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>LogisticRegression</td>\n",
       "      <td>1</td>\n",
       "      <td>0.822581</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>LogisticRegression</td>\n",
       "      <td>2</td>\n",
       "      <td>0.778226</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>LogisticRegression</td>\n",
       "      <td>3</td>\n",
       "      <td>0.814516</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>LogisticRegression</td>\n",
       "      <td>4</td>\n",
       "      <td>0.754032</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>LogisticRegression</td>\n",
       "      <td>5</td>\n",
       "      <td>0.794355</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6</td>\n",
       "      <td>LogisticRegression</td>\n",
       "      <td>6</td>\n",
       "      <td>0.838710</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7</td>\n",
       "      <td>LogisticRegression</td>\n",
       "      <td>7</td>\n",
       "      <td>0.818548</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8</td>\n",
       "      <td>LogisticRegression</td>\n",
       "      <td>8</td>\n",
       "      <td>0.774194</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9</td>\n",
       "      <td>LogisticRegression</td>\n",
       "      <td>9</td>\n",
       "      <td>0.774194</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "           model_name  fold_idx  accuracy\n",
       "0  LogisticRegression         0  0.802419\n",
       "1  LogisticRegression         1  0.822581\n",
       "2  LogisticRegression         2  0.778226\n",
       "3  LogisticRegression         3  0.814516\n",
       "4  LogisticRegression         4  0.754032\n",
       "5  LogisticRegression         5  0.794355\n",
       "6  LogisticRegression         6  0.838710\n",
       "7  LogisticRegression         7  0.818548\n",
       "8  LogisticRegression         8  0.774194\n",
       "9  LogisticRegression         9  0.774194"
      ]
     },
     "execution_count": 170,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "logistic_regression_model = LogisticRegression(max_iter=7500)\n",
    "\n",
    "CV = 10\n",
    "cv_df = pd.DataFrame(index=range(CV * 1))\n",
    "entries = []\n",
    "\n",
    "model_name = logistic_regression_model.__class__.__name__\n",
    "accuracies = cross_val_score(logistic_regression_model, x_train_tfidf, y_train_tfidf, scoring='accuracy', cv=CV)\n",
    "for fold_idx, accuracy in enumerate(accuracies):\n",
    "    entries.append((model_name, fold_idx, accuracy))\n",
    "cv_df = pd.DataFrame(entries, columns=['model_name', 'fold_idx', 'accuracy'])\n",
    "\n",
    "cv_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 171,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LogisticRegression\n",
      "[[228  77]\n",
      " [ 52 263]]\n",
      "Accuracy: 0.7919354838709678\n",
      "Macro Precision: 0.7939075630252102\n",
      "Macro Recall: 0.7912308092635962\n",
      "Macro F1 score:0.7912703073008416\n",
      "MCC:0.5851322497641822\n"
     ]
    }
   ],
   "source": [
    "#Measure performance on validation data\n",
    "logistic_regression_model.fit(x_train_tfidf, y_train_tfidf)\n",
    "print(model_name)\n",
    "# Do the prediction\n",
    "y_predict_tfidf=logistic_regression_model.predict(x_valid_tfidf)\n",
    "print(confusion_matrix(y_valid_tfidf,y_predict_tfidf))\n",
    "recall=recall_score(y_valid_tfidf,y_predict_tfidf,average='macro')\n",
    "precision=precision_score(y_valid_tfidf,y_predict_tfidf,average='macro')\n",
    "f1score=f1_score(y_valid_tfidf,y_predict_tfidf,average='macro')\n",
    "accuracy=accuracy_score(y_valid_tfidf,y_predict_tfidf)\n",
    "matthews = matthews_corrcoef(y_valid_tfidf,y_predict_tfidf) \n",
    "print('Accuracy: '+ str(accuracy))\n",
    "print('Macro Precision: '+ str(precision))\n",
    "print('Macro Recall: '+ str(recall))\n",
    "print('Macro F1 score:'+ str(f1score))\n",
    "print('MCC:'+ str(matthews))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### LOGISTIC WITH GLOVE EMBEDDINGS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 167,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>model_name</th>\n",
       "      <th>fold_idx</th>\n",
       "      <th>accuracy</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>LogisticRegression</td>\n",
       "      <td>0</td>\n",
       "      <td>0.766129</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>LogisticRegression</td>\n",
       "      <td>1</td>\n",
       "      <td>0.814516</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>LogisticRegression</td>\n",
       "      <td>2</td>\n",
       "      <td>0.786290</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>LogisticRegression</td>\n",
       "      <td>3</td>\n",
       "      <td>0.802419</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>LogisticRegression</td>\n",
       "      <td>4</td>\n",
       "      <td>0.721774</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>LogisticRegression</td>\n",
       "      <td>5</td>\n",
       "      <td>0.745968</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6</td>\n",
       "      <td>LogisticRegression</td>\n",
       "      <td>6</td>\n",
       "      <td>0.842742</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7</td>\n",
       "      <td>LogisticRegression</td>\n",
       "      <td>7</td>\n",
       "      <td>0.798387</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8</td>\n",
       "      <td>LogisticRegression</td>\n",
       "      <td>8</td>\n",
       "      <td>0.790323</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9</td>\n",
       "      <td>LogisticRegression</td>\n",
       "      <td>9</td>\n",
       "      <td>0.790323</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "           model_name  fold_idx  accuracy\n",
       "0  LogisticRegression         0  0.766129\n",
       "1  LogisticRegression         1  0.814516\n",
       "2  LogisticRegression         2  0.786290\n",
       "3  LogisticRegression         3  0.802419\n",
       "4  LogisticRegression         4  0.721774\n",
       "5  LogisticRegression         5  0.745968\n",
       "6  LogisticRegression         6  0.842742\n",
       "7  LogisticRegression         7  0.798387\n",
       "8  LogisticRegression         8  0.790323\n",
       "9  LogisticRegression         9  0.790323"
      ]
     },
     "execution_count": 167,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "logistic_regression_model_glove = LogisticRegression(max_iter=7500)\n",
    "\n",
    "CV = 10\n",
    "cv_df = pd.DataFrame(index=range(CV * 1))\n",
    "entries = []\n",
    "\n",
    "model_name = logistic_regression_model_glove.__class__.__name__\n",
    "accuracies = cross_val_score(logistic_regression_model_glove, x_train_w2vec, y_train_w2vec, scoring='accuracy', cv=CV)\n",
    "for fold_idx, accuracy in enumerate(accuracies):\n",
    "    entries.append((model_name, fold_idx, accuracy))\n",
    "cv_df = pd.DataFrame(entries, columns=['model_name', 'fold_idx', 'accuracy'])\n",
    "\n",
    "cv_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 172,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LogisticRegression\n",
      "[[240  65]\n",
      " [ 62 253]]\n",
      "Accuracy: 0.7951612903225806\n",
      "Macro Precision: 0.7951497355158482\n",
      "Macro Recall: 0.7950299245381213\n",
      "Macro F1 score:0.795071194151435\n",
      "MCC:0.5901796478926976\n"
     ]
    }
   ],
   "source": [
    "#Measure performance on validation data\n",
    "logistic_regression_model_glove.fit(x_train_w2vec, y_train_w2vec)\n",
    "print(model_name)\n",
    "# Do the prediction\n",
    "y_predict_w2vec=logistic_regression_model_glove.predict(x_valid_w2vec)\n",
    "print(confusion_matrix(y_valid_w2vec,y_predict_w2vec))\n",
    "recall=recall_score(y_valid_w2vec,y_predict_w2vec,average='macro')\n",
    "precision=precision_score(y_valid_w2vec,y_predict_w2vec,average='macro')\n",
    "f1score=f1_score(y_valid_w2vec,y_predict_w2vec,average='macro')\n",
    "accuracy=accuracy_score(y_valid_w2vec,y_predict_w2vec)\n",
    "matthews = matthews_corrcoef(y_valid_w2vec,y_predict_w2vec) \n",
    "print('Accuracy: '+ str(accuracy))\n",
    "print('Macro Precision: '+ str(precision))\n",
    "print('Macro Recall: '+ str(recall))\n",
    "print('Macro F1 score:'+ str(f1score))\n",
    "print('MCC:'+ str(matthews))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### NAIVE BAYES CLASSIFIER"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>model_name</th>\n",
       "      <th>fold_idx</th>\n",
       "      <th>accuracy</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>MultinomialNB</td>\n",
       "      <td>0</td>\n",
       "      <td>0.762097</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>MultinomialNB</td>\n",
       "      <td>1</td>\n",
       "      <td>0.762097</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>MultinomialNB</td>\n",
       "      <td>2</td>\n",
       "      <td>0.762097</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>MultinomialNB</td>\n",
       "      <td>3</td>\n",
       "      <td>0.790323</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>MultinomialNB</td>\n",
       "      <td>4</td>\n",
       "      <td>0.685484</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>MultinomialNB</td>\n",
       "      <td>5</td>\n",
       "      <td>0.766129</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6</td>\n",
       "      <td>MultinomialNB</td>\n",
       "      <td>6</td>\n",
       "      <td>0.733871</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7</td>\n",
       "      <td>MultinomialNB</td>\n",
       "      <td>7</td>\n",
       "      <td>0.778226</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8</td>\n",
       "      <td>MultinomialNB</td>\n",
       "      <td>8</td>\n",
       "      <td>0.754032</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9</td>\n",
       "      <td>MultinomialNB</td>\n",
       "      <td>9</td>\n",
       "      <td>0.725806</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      model_name  fold_idx  accuracy\n",
       "0  MultinomialNB         0  0.762097\n",
       "1  MultinomialNB         1  0.762097\n",
       "2  MultinomialNB         2  0.762097\n",
       "3  MultinomialNB         3  0.790323\n",
       "4  MultinomialNB         4  0.685484\n",
       "5  MultinomialNB         5  0.766129\n",
       "6  MultinomialNB         6  0.733871\n",
       "7  MultinomialNB         7  0.778226\n",
       "8  MultinomialNB         8  0.754032\n",
       "9  MultinomialNB         9  0.725806"
      ]
     },
     "execution_count": 163,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "naive_bayes_classifier = MultinomialNB()\n",
    "\n",
    "CV = 10\n",
    "cv_df = pd.DataFrame(index=range(CV * 1))\n",
    "entries = []\n",
    "\n",
    "model_name = naive_bayes_classifier.__class__.__name__\n",
    "accuracies = cross_val_score(naive_bayes_classifier, x_train_tfidf, y_train_tfidf, scoring='accuracy', cv=CV)\n",
    "for fold_idx, accuracy in enumerate(accuracies):\n",
    "    entries.append((model_name, fold_idx, accuracy))\n",
    "cv_df = pd.DataFrame(entries, columns=['model_name', 'fold_idx', 'accuracy'])\n",
    "\n",
    "cv_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MultinomialNB\n",
      "[[230  75]\n",
      " [ 74 241]]\n",
      "Accuracy: 0.7596774193548387\n",
      "Macro Precision: 0.7596185876082611\n",
      "Macro Recall: 0.7595888628675513\n",
      "Macro F1 score:0.7596017476885283\n",
      "MCC:0.5192074496249386\n"
     ]
    }
   ],
   "source": [
    "#Measure performance on validation data\n",
    "naive_bayes_classifier.fit(x_train_tfidf, y_train_tfidf)\n",
    "print(model_name)\n",
    "# Do the prediction\n",
    "y_predict_tfidf=naive_bayes_classifier.predict(x_valid_tfidf)\n",
    "print(confusion_matrix(y_valid_tfidf,y_predict_tfidf))\n",
    "recall=recall_score(y_valid_tfidf,y_predict_tfidf,average='macro')\n",
    "precision=precision_score(y_valid_tfidf,y_predict_tfidf,average='macro')\n",
    "f1score=f1_score(y_valid_tfidf,y_predict_tfidf,average='macro')\n",
    "accuracy=accuracy_score(y_valid_tfidf,y_predict_tfidf)\n",
    "matthews = matthews_corrcoef(y_valid_tfidf,y_predict_tfidf) \n",
    "print('Accuracy: '+ str(accuracy))\n",
    "print('Macro Precision: '+ str(precision))\n",
    "print('Macro Recall: '+ str(recall))\n",
    "print('Macro F1 score:'+ str(f1score))\n",
    "print('MCC:'+ str(matthews))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### RANDOM FOREST"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>model_name</th>\n",
       "      <th>fold_idx</th>\n",
       "      <th>accuracy</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>RandomForestClassifier</td>\n",
       "      <td>0</td>\n",
       "      <td>0.758065</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>RandomForestClassifier</td>\n",
       "      <td>1</td>\n",
       "      <td>0.745968</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>RandomForestClassifier</td>\n",
       "      <td>2</td>\n",
       "      <td>0.758065</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>RandomForestClassifier</td>\n",
       "      <td>3</td>\n",
       "      <td>0.770161</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>RandomForestClassifier</td>\n",
       "      <td>4</td>\n",
       "      <td>0.681452</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>RandomForestClassifier</td>\n",
       "      <td>5</td>\n",
       "      <td>0.750000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6</td>\n",
       "      <td>RandomForestClassifier</td>\n",
       "      <td>6</td>\n",
       "      <td>0.778226</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7</td>\n",
       "      <td>RandomForestClassifier</td>\n",
       "      <td>7</td>\n",
       "      <td>0.786290</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8</td>\n",
       "      <td>RandomForestClassifier</td>\n",
       "      <td>8</td>\n",
       "      <td>0.750000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9</td>\n",
       "      <td>RandomForestClassifier</td>\n",
       "      <td>9</td>\n",
       "      <td>0.713710</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "               model_name  fold_idx  accuracy\n",
       "0  RandomForestClassifier         0  0.758065\n",
       "1  RandomForestClassifier         1  0.745968\n",
       "2  RandomForestClassifier         2  0.758065\n",
       "3  RandomForestClassifier         3  0.770161\n",
       "4  RandomForestClassifier         4  0.681452\n",
       "5  RandomForestClassifier         5  0.750000\n",
       "6  RandomForestClassifier         6  0.778226\n",
       "7  RandomForestClassifier         7  0.786290\n",
       "8  RandomForestClassifier         8  0.750000\n",
       "9  RandomForestClassifier         9  0.713710"
      ]
     },
     "execution_count": 103,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "random_forest_classifier = RandomForestClassifier(max_depth=5, random_state=0)\n",
    "\n",
    "CV = 10\n",
    "cv_df = pd.DataFrame(index=range(CV * 1))\n",
    "entries = []\n",
    "\n",
    "model_name = random_forest_classifier.__class__.__name__\n",
    "accuracies = cross_val_score(random_forest_classifier, x_train_w2vec, y_train_w2vec, scoring='accuracy', cv=CV)\n",
    "for fold_idx, accuracy in enumerate(accuracies):\n",
    "    entries.append((model_name, fold_idx, accuracy))\n",
    "cv_df = pd.DataFrame(entries, columns=['model_name', 'fold_idx', 'accuracy'])\n",
    "\n",
    "cv_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RandomForestClassifier\n",
      "[[213  92]\n",
      " [ 64 251]]\n",
      "Accuracy: 0.7483870967741936\n",
      "Macro Precision: 0.7503657471240173\n",
      "Macro Recall: 0.7475930262815509\n",
      "Macro F1 score:0.7474383480086485\n",
      "MCC:0.49795105385038363\n"
     ]
    }
   ],
   "source": [
    "#Measure performance on validation data\n",
    "random_forest_classifier.fit(x_train_w2vec, y_train_w2vec)\n",
    "print(model_name)\n",
    "# Do the prediction\n",
    "y_predict_w2vec=random_forest_classifier.predict(x_valid_w2vec)\n",
    "print(confusion_matrix(y_valid_w2vec,y_predict_w2vec))\n",
    "recall=recall_score(y_valid_w2vec,y_predict_w2vec,average='macro')\n",
    "precision=precision_score(y_valid_w2vec,y_predict_w2vec,average='macro')\n",
    "f1score=f1_score(y_valid_w2vec,y_predict_w2vec,average='macro')\n",
    "accuracy=accuracy_score(y_valid_w2vec,y_predict_w2vec)\n",
    "matthews = matthews_corrcoef(y_valid_w2vec,y_predict_w2vec) \n",
    "print('Accuracy: '+ str(accuracy))\n",
    "print('Macro Precision: '+ str(precision))\n",
    "print('Macro Recall: '+ str(recall))\n",
    "print('Macro F1 score:'+ str(f1score))\n",
    "print('MCC:'+ str(matthews))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### SUPPORT VECTOR MACHINES WITH GLOVE EMBEDDINGS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>model_name</th>\n",
       "      <th>fold_idx</th>\n",
       "      <th>accuracy</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>SVC</td>\n",
       "      <td>0</td>\n",
       "      <td>0.794355</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>SVC</td>\n",
       "      <td>1</td>\n",
       "      <td>0.806452</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>SVC</td>\n",
       "      <td>2</td>\n",
       "      <td>0.810484</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>SVC</td>\n",
       "      <td>3</td>\n",
       "      <td>0.798387</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>SVC</td>\n",
       "      <td>4</td>\n",
       "      <td>0.758065</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>SVC</td>\n",
       "      <td>5</td>\n",
       "      <td>0.766129</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6</td>\n",
       "      <td>SVC</td>\n",
       "      <td>6</td>\n",
       "      <td>0.846774</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7</td>\n",
       "      <td>SVC</td>\n",
       "      <td>7</td>\n",
       "      <td>0.826613</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8</td>\n",
       "      <td>SVC</td>\n",
       "      <td>8</td>\n",
       "      <td>0.822581</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9</td>\n",
       "      <td>SVC</td>\n",
       "      <td>9</td>\n",
       "      <td>0.778226</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  model_name  fold_idx  accuracy\n",
       "0        SVC         0  0.794355\n",
       "1        SVC         1  0.806452\n",
       "2        SVC         2  0.810484\n",
       "3        SVC         3  0.798387\n",
       "4        SVC         4  0.758065\n",
       "5        SVC         5  0.766129\n",
       "6        SVC         6  0.846774\n",
       "7        SVC         7  0.826613\n",
       "8        SVC         8  0.822581\n",
       "9        SVC         9  0.778226"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "svm_classifier = SVC(C=1.0, kernel='rbf', degree=3, gamma='auto')\n",
    "\n",
    "CV = 10\n",
    "cv_df = pd.DataFrame(index=range(CV * 1))\n",
    "entries = []\n",
    "\n",
    "model_name = svm_classifier.__class__.__name__\n",
    "accuracies = cross_val_score(svm_classifier, x_train_w2vec, y_train_w2vec, scoring='accuracy', cv=CV)\n",
    "for fold_idx, accuracy in enumerate(accuracies):\n",
    "    entries.append((model_name, fold_idx, accuracy))\n",
    "cv_df = pd.DataFrame(entries, columns=['model_name', 'fold_idx', 'accuracy'])\n",
    "\n",
    "cv_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SVC\n",
      "[[241  64]\n",
      " [ 56 259]]\n",
      "Accuracy: 0.8064516129032258\n",
      "Macro Precision: 0.8066526982935651\n",
      "Macro Recall: 0.8061930783242258\n",
      "Macro F1 score:0.8062883387662858\n",
      "MCC:0.6128456042656644\n"
     ]
    }
   ],
   "source": [
    "#Measure performance on validation data\n",
    "svm_classifier.fit(x_train_w2vec, y_train_w2vec)\n",
    "print(model_name)\n",
    "# Do the prediction\n",
    "y_predict_w2vec=svm_classifier.predict(x_valid_w2vec)\n",
    "print(confusion_matrix(y_valid_w2vec,y_predict_w2vec))\n",
    "recall=recall_score(y_valid_w2vec,y_predict_w2vec,average='macro')\n",
    "precision=precision_score(y_valid_w2vec,y_predict_w2vec,average='macro')\n",
    "f1score=f1_score(y_valid_w2vec,y_predict_w2vec,average='macro')\n",
    "accuracy=accuracy_score(y_valid_w2vec,y_predict_w2vec)\n",
    "matthews = matthews_corrcoef(y_valid_w2vec,y_predict_w2vec) \n",
    "print('Accuracy: '+ str(accuracy))\n",
    "print('Macro Precision: '+ str(precision))\n",
    "print('Macro Recall: '+ str(recall))\n",
    "print('Macro F1 score:'+ str(f1score))\n",
    "print('MCC:'+ str(matthews))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Multilayer Perceptron"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>model_name</th>\n",
       "      <th>fold_idx</th>\n",
       "      <th>accuracy</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>MLPClassifier</td>\n",
       "      <td>0</td>\n",
       "      <td>0.782258</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>MLPClassifier</td>\n",
       "      <td>1</td>\n",
       "      <td>0.786290</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>MLPClassifier</td>\n",
       "      <td>2</td>\n",
       "      <td>0.798387</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>MLPClassifier</td>\n",
       "      <td>3</td>\n",
       "      <td>0.806452</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>MLPClassifier</td>\n",
       "      <td>4</td>\n",
       "      <td>0.733871</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>MLPClassifier</td>\n",
       "      <td>5</td>\n",
       "      <td>0.737903</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6</td>\n",
       "      <td>MLPClassifier</td>\n",
       "      <td>6</td>\n",
       "      <td>0.810484</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7</td>\n",
       "      <td>MLPClassifier</td>\n",
       "      <td>7</td>\n",
       "      <td>0.782258</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8</td>\n",
       "      <td>MLPClassifier</td>\n",
       "      <td>8</td>\n",
       "      <td>0.774194</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9</td>\n",
       "      <td>MLPClassifier</td>\n",
       "      <td>9</td>\n",
       "      <td>0.782258</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      model_name  fold_idx  accuracy\n",
       "0  MLPClassifier         0  0.782258\n",
       "1  MLPClassifier         1  0.786290\n",
       "2  MLPClassifier         2  0.798387\n",
       "3  MLPClassifier         3  0.806452\n",
       "4  MLPClassifier         4  0.733871\n",
       "5  MLPClassifier         5  0.737903\n",
       "6  MLPClassifier         6  0.810484\n",
       "7  MLPClassifier         7  0.782258\n",
       "8  MLPClassifier         8  0.774194\n",
       "9  MLPClassifier         9  0.782258"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.neural_network import MLPClassifier\n",
    "mlp_classifier = MLPClassifier(random_state=1, max_iter=300,  solver='adam',  activation='relu', hidden_layer_sizes=(126, ), learning_rate_init=0.001)\n",
    "\n",
    "CV = 10\n",
    "cv_df = pd.DataFrame(index=range(CV * 1))\n",
    "entries = []\n",
    "\n",
    "model_name = mlp_classifier.__class__.__name__\n",
    "accuracies = cross_val_score(mlp_classifier, x_train_w2vec, y_train_w2vec, scoring='accuracy', cv=CV)\n",
    "for fold_idx, accuracy in enumerate(accuracies):\n",
    "    entries.append((model_name, fold_idx, accuracy))\n",
    "cv_df = pd.DataFrame(entries, columns=['model_name', 'fold_idx', 'accuracy'])\n",
    "\n",
    "cv_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MLPClassifier\n",
      "[[247  58]\n",
      " [ 62 253]]\n",
      "Accuracy: 0.8064516129032258\n",
      "Macro Precision: 0.8064287869800935\n",
      "Macro Recall: 0.8065053343741868\n",
      "Macro F1 score:0.8064334849257475\n",
      "MCC:0.6129341165743999\n"
     ]
    }
   ],
   "source": [
    "#Measure performance on validation data\n",
    "mlp_classifier.fit(x_train_w2vec, y_train_w2vec)\n",
    "print(model_name)\n",
    "# Do the prediction\n",
    "y_predict_w2vec=mlp_classifier.predict(x_valid_w2vec)\n",
    "print(confusion_matrix(y_valid_w2vec,y_predict_w2vec))\n",
    "recall=recall_score(y_valid_w2vec,y_predict_w2vec,average='macro')\n",
    "precision=precision_score(y_valid_w2vec,y_predict_w2vec,average='macro')\n",
    "f1score=f1_score(y_valid_w2vec,y_predict_w2vec,average='macro')\n",
    "accuracy=accuracy_score(y_valid_w2vec,y_predict_w2vec)\n",
    "matthews = matthews_corrcoef(y_valid_w2vec,y_predict_w2vec) \n",
    "print('Accuracy: '+ str(accuracy))\n",
    "print('Macro Precision: '+ str(precision))\n",
    "print('Macro Recall: '+ str(recall))\n",
    "print('Macro F1 score:'+ str(f1score))\n",
    "print('MCC:'+ str(matthews))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Stacked Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>model_name</th>\n",
       "      <th>fold_idx</th>\n",
       "      <th>accuracy</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>MLPClassifier</td>\n",
       "      <td>0</td>\n",
       "      <td>0.782258</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>MLPClassifier</td>\n",
       "      <td>1</td>\n",
       "      <td>0.786290</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>MLPClassifier</td>\n",
       "      <td>2</td>\n",
       "      <td>0.798387</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>MLPClassifier</td>\n",
       "      <td>3</td>\n",
       "      <td>0.806452</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>MLPClassifier</td>\n",
       "      <td>4</td>\n",
       "      <td>0.733871</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>MLPClassifier</td>\n",
       "      <td>5</td>\n",
       "      <td>0.737903</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6</td>\n",
       "      <td>MLPClassifier</td>\n",
       "      <td>6</td>\n",
       "      <td>0.810484</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7</td>\n",
       "      <td>MLPClassifier</td>\n",
       "      <td>7</td>\n",
       "      <td>0.782258</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8</td>\n",
       "      <td>MLPClassifier</td>\n",
       "      <td>8</td>\n",
       "      <td>0.774194</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9</td>\n",
       "      <td>MLPClassifier</td>\n",
       "      <td>9</td>\n",
       "      <td>0.782258</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10</td>\n",
       "      <td>StackingClassifier</td>\n",
       "      <td>0</td>\n",
       "      <td>0.790323</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>11</td>\n",
       "      <td>StackingClassifier</td>\n",
       "      <td>1</td>\n",
       "      <td>0.810484</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>12</td>\n",
       "      <td>StackingClassifier</td>\n",
       "      <td>2</td>\n",
       "      <td>0.814516</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>13</td>\n",
       "      <td>StackingClassifier</td>\n",
       "      <td>3</td>\n",
       "      <td>0.822581</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>14</td>\n",
       "      <td>StackingClassifier</td>\n",
       "      <td>4</td>\n",
       "      <td>0.754032</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>15</td>\n",
       "      <td>StackingClassifier</td>\n",
       "      <td>5</td>\n",
       "      <td>0.762097</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>16</td>\n",
       "      <td>StackingClassifier</td>\n",
       "      <td>6</td>\n",
       "      <td>0.834677</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>17</td>\n",
       "      <td>StackingClassifier</td>\n",
       "      <td>7</td>\n",
       "      <td>0.822581</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>18</td>\n",
       "      <td>StackingClassifier</td>\n",
       "      <td>8</td>\n",
       "      <td>0.814516</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>19</td>\n",
       "      <td>StackingClassifier</td>\n",
       "      <td>9</td>\n",
       "      <td>0.782258</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>20</td>\n",
       "      <td>StackingClassifier</td>\n",
       "      <td>0</td>\n",
       "      <td>0.790323</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>21</td>\n",
       "      <td>StackingClassifier</td>\n",
       "      <td>1</td>\n",
       "      <td>0.810484</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>22</td>\n",
       "      <td>StackingClassifier</td>\n",
       "      <td>2</td>\n",
       "      <td>0.814516</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>23</td>\n",
       "      <td>StackingClassifier</td>\n",
       "      <td>3</td>\n",
       "      <td>0.822581</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>24</td>\n",
       "      <td>StackingClassifier</td>\n",
       "      <td>4</td>\n",
       "      <td>0.750000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>25</td>\n",
       "      <td>StackingClassifier</td>\n",
       "      <td>5</td>\n",
       "      <td>0.758065</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>26</td>\n",
       "      <td>StackingClassifier</td>\n",
       "      <td>6</td>\n",
       "      <td>0.834677</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>27</td>\n",
       "      <td>StackingClassifier</td>\n",
       "      <td>7</td>\n",
       "      <td>0.822581</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>28</td>\n",
       "      <td>StackingClassifier</td>\n",
       "      <td>8</td>\n",
       "      <td>0.814516</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>29</td>\n",
       "      <td>StackingClassifier</td>\n",
       "      <td>9</td>\n",
       "      <td>0.770161</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "            model_name  fold_idx  accuracy\n",
       "0        MLPClassifier         0  0.782258\n",
       "1        MLPClassifier         1  0.786290\n",
       "2        MLPClassifier         2  0.798387\n",
       "3        MLPClassifier         3  0.806452\n",
       "4        MLPClassifier         4  0.733871\n",
       "5        MLPClassifier         5  0.737903\n",
       "6        MLPClassifier         6  0.810484\n",
       "7        MLPClassifier         7  0.782258\n",
       "8        MLPClassifier         8  0.774194\n",
       "9        MLPClassifier         9  0.782258\n",
       "10  StackingClassifier         0  0.790323\n",
       "11  StackingClassifier         1  0.810484\n",
       "12  StackingClassifier         2  0.814516\n",
       "13  StackingClassifier         3  0.822581\n",
       "14  StackingClassifier         4  0.754032\n",
       "15  StackingClassifier         5  0.762097\n",
       "16  StackingClassifier         6  0.834677\n",
       "17  StackingClassifier         7  0.822581\n",
       "18  StackingClassifier         8  0.814516\n",
       "19  StackingClassifier         9  0.782258\n",
       "20  StackingClassifier         0  0.790323\n",
       "21  StackingClassifier         1  0.810484\n",
       "22  StackingClassifier         2  0.814516\n",
       "23  StackingClassifier         3  0.822581\n",
       "24  StackingClassifier         4  0.750000\n",
       "25  StackingClassifier         5  0.758065\n",
       "26  StackingClassifier         6  0.834677\n",
       "27  StackingClassifier         7  0.822581\n",
       "28  StackingClassifier         8  0.814516\n",
       "29  StackingClassifier         9  0.770161"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clf1 = SVC(C=1.0, kernel='rbf', degree=3, gamma='auto')\n",
    "clf2 = RandomForestClassifier(max_depth=5, random_state=0)\n",
    "clf3 = MLPClassifier(random_state=1, max_iter=300,  solver='adam',  activation='relu', hidden_layer_sizes=(126, ), learning_rate_init=0.001)\n",
    "lr = LogisticRegression()\n",
    "\n",
    "\n",
    "base_learners = [\n",
    "                    ('clf1', SVC(C=1.0, kernel='rbf', degree=3, gamma='auto')),\n",
    "#                     ('clf2', RandomForestClassifier(max_depth=5, random_state=0)),\n",
    "                    ('clf3', MLPClassifier(random_state=1, max_iter=300,  solver='adam',  activation='relu', hidden_layer_sizes=(126, ), learning_rate_init=0.001))\n",
    "                ]\n",
    "\n",
    "sclf = StackingClassifier(estimators=base_learners, final_estimator=LogisticRegression())\n",
    "\n",
    "model_name = sclf.__class__.__name__\n",
    "accuracies = cross_val_score(sclf, x_train_w2vec, y_train_w2vec, scoring='accuracy', cv=CV)\n",
    "for fold_idx, accuracy in enumerate(accuracies):\n",
    "    entries.append((model_name, fold_idx, accuracy))\n",
    "cv_df = pd.DataFrame(entries, columns=['model_name', 'fold_idx', 'accuracy'])\n",
    "\n",
    "cv_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "StackingClassifier\n",
      "[[243  62]\n",
      " [ 57 258]]\n",
      "Accuracy: 0.8080645161290323\n",
      "Macro Precision: 0.808125\n",
      "Macro Recall: 0.8078844652615145\n",
      "Macro F1 score:0.8079521051604086\n",
      "MCC:0.6160094183004158\n"
     ]
    }
   ],
   "source": [
    "#Measure performance on validation data\n",
    "sclf.fit(x_train_w2vec, y_train_w2vec)\n",
    "print(model_name)\n",
    "# Do the prediction\n",
    "y_predict_w2vec=sclf.predict(x_valid_w2vec)\n",
    "print(confusion_matrix(y_valid_w2vec,y_predict_w2vec))\n",
    "recall=recall_score(y_valid_w2vec,y_predict_w2vec,average='macro')\n",
    "precision=precision_score(y_valid_w2vec,y_predict_w2vec,average='macro')\n",
    "f1score=f1_score(y_valid_w2vec,y_predict_w2vec,average='macro')\n",
    "accuracy=accuracy_score(y_valid_w2vec,y_predict_w2vec)\n",
    "matthews = matthews_corrcoef(y_valid_w2vec,y_predict_w2vec) \n",
    "print('Accuracy: '+ str(accuracy))\n",
    "print('Macro Precision: '+ str(precision))\n",
    "print('Macro Recall: '+ str(recall))\n",
    "print('Macro F1 score:'+ str(f1score))\n",
    "print('MCC:'+ str(matthews))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
